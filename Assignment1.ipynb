{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bhfIKbQzGq2"
      },
      "source": [
        "# Assignment 1. Music Century Classification\n",
        "\n",
        "**Assignment Responsible**: Natalie Lang.\n",
        "\n",
        "In this assignment, we will build models to predict which\n",
        "**century** a piece of music was released.  We will be using the \"YearPredictionMSD Data Set\"\n",
        "based on the Million Song Dataset. The data is available to download from the UCI \n",
        "Machine Learning Repository. Here are some links about the data:\n",
        "\n",
        "- https://archive.ics.uci.edu/ml/datasets/yearpredictionmsd\n",
        "- http://millionsongdataset.com/pages/tasks-demos/#yearrecognition\n",
        "\n",
        "Note that you are note allowed to import additional packages **(especially not PyTorch)**. One of the objectives is to understand how the training procedure actually operates, before working with PyTorch's autograd engine which does it all for us.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47oq1vy5PUIV"
      },
      "source": [
        "## Question 1. Data (21%)\n",
        "\n",
        "Start by setting up a Google Colab notebook in which to do your work.\n",
        "Since you are working with a partner, you might find this link helpful:\n",
        "\n",
        "- https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb\n",
        "\n",
        "The recommended way to work together is pair coding, where you and your partner are sitting together and writing code together. \n",
        "\n",
        "To process and read the data, we use the popular `pandas` package for data analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aFWpuNSzGq9"
      },
      "source": [
        "import pandas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(1) # To reproduce our experiment"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7UWL6mFzGq-"
      },
      "source": [
        "Now that your notebook is set up, we can load the data into the notebook. The code below provides\n",
        "two ways of loading the data: directly from the internet, or through mounting Google Drive.\n",
        "The first method is easier but slower, and the second method is a bit involved at first, but\n",
        "can save you time later on. You will need to mount Google Drive for later assignments, so we recommend\n",
        "figuring how to do that now.\n",
        "\n",
        "Here are some resources to help you get started:\n",
        "\n",
        "- http.://colab.research.google.com/notebooks/io.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EY6PrfV4zGq_"
      },
      "source": [
        "load_from_drive = False\n",
        "\n",
        "if not load_from_drive:\n",
        "  csv_path = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip\"\n",
        "else:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  csv_path = '/content/gdrive/My Drive/YearPredictionMSD.txt.zip' # TODO - UPDATE ME WITH THE TRUE PATH!\n",
        "\n",
        "t_label = [\"year\"]\n",
        "x_labels = [\"var%d\" % i for i in range(1, 91)]\n",
        "df = pandas.read_csv(csv_path, names=t_label + x_labels)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgB83beNzGq_"
      },
      "source": [
        "Now that the data is loaded to your Colab notebook, you should be able to display the Pandas\n",
        "DataFrame `df` as a table:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5bBEnj3zGq_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "aac3c08f-1af1-4a67-ee7d-2661254a1597"
      },
      "source": [
        "df"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>var1</th>\n",
              "      <th>var2</th>\n",
              "      <th>var3</th>\n",
              "      <th>var4</th>\n",
              "      <th>var5</th>\n",
              "      <th>var6</th>\n",
              "      <th>var7</th>\n",
              "      <th>var8</th>\n",
              "      <th>var9</th>\n",
              "      <th>var10</th>\n",
              "      <th>var11</th>\n",
              "      <th>var12</th>\n",
              "      <th>var13</th>\n",
              "      <th>var14</th>\n",
              "      <th>var15</th>\n",
              "      <th>var16</th>\n",
              "      <th>var17</th>\n",
              "      <th>var18</th>\n",
              "      <th>var19</th>\n",
              "      <th>var20</th>\n",
              "      <th>var21</th>\n",
              "      <th>var22</th>\n",
              "      <th>var23</th>\n",
              "      <th>var24</th>\n",
              "      <th>var25</th>\n",
              "      <th>var26</th>\n",
              "      <th>var27</th>\n",
              "      <th>var28</th>\n",
              "      <th>var29</th>\n",
              "      <th>var30</th>\n",
              "      <th>var31</th>\n",
              "      <th>var32</th>\n",
              "      <th>var33</th>\n",
              "      <th>var34</th>\n",
              "      <th>var35</th>\n",
              "      <th>var36</th>\n",
              "      <th>var37</th>\n",
              "      <th>var38</th>\n",
              "      <th>var39</th>\n",
              "      <th>...</th>\n",
              "      <th>var51</th>\n",
              "      <th>var52</th>\n",
              "      <th>var53</th>\n",
              "      <th>var54</th>\n",
              "      <th>var55</th>\n",
              "      <th>var56</th>\n",
              "      <th>var57</th>\n",
              "      <th>var58</th>\n",
              "      <th>var59</th>\n",
              "      <th>var60</th>\n",
              "      <th>var61</th>\n",
              "      <th>var62</th>\n",
              "      <th>var63</th>\n",
              "      <th>var64</th>\n",
              "      <th>var65</th>\n",
              "      <th>var66</th>\n",
              "      <th>var67</th>\n",
              "      <th>var68</th>\n",
              "      <th>var69</th>\n",
              "      <th>var70</th>\n",
              "      <th>var71</th>\n",
              "      <th>var72</th>\n",
              "      <th>var73</th>\n",
              "      <th>var74</th>\n",
              "      <th>var75</th>\n",
              "      <th>var76</th>\n",
              "      <th>var77</th>\n",
              "      <th>var78</th>\n",
              "      <th>var79</th>\n",
              "      <th>var80</th>\n",
              "      <th>var81</th>\n",
              "      <th>var82</th>\n",
              "      <th>var83</th>\n",
              "      <th>var84</th>\n",
              "      <th>var85</th>\n",
              "      <th>var86</th>\n",
              "      <th>var87</th>\n",
              "      <th>var88</th>\n",
              "      <th>var89</th>\n",
              "      <th>var90</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2001</td>\n",
              "      <td>49.94357</td>\n",
              "      <td>21.47114</td>\n",
              "      <td>73.07750</td>\n",
              "      <td>8.74861</td>\n",
              "      <td>-17.40628</td>\n",
              "      <td>-13.09905</td>\n",
              "      <td>-25.01202</td>\n",
              "      <td>-12.23257</td>\n",
              "      <td>7.83089</td>\n",
              "      <td>-2.46783</td>\n",
              "      <td>3.32136</td>\n",
              "      <td>-2.31521</td>\n",
              "      <td>10.20556</td>\n",
              "      <td>611.10913</td>\n",
              "      <td>951.08960</td>\n",
              "      <td>698.11428</td>\n",
              "      <td>408.98485</td>\n",
              "      <td>383.70912</td>\n",
              "      <td>326.51512</td>\n",
              "      <td>238.11327</td>\n",
              "      <td>251.42414</td>\n",
              "      <td>187.17351</td>\n",
              "      <td>100.42652</td>\n",
              "      <td>179.19498</td>\n",
              "      <td>-8.41558</td>\n",
              "      <td>-317.87038</td>\n",
              "      <td>95.86266</td>\n",
              "      <td>48.10259</td>\n",
              "      <td>-95.66303</td>\n",
              "      <td>-18.06215</td>\n",
              "      <td>1.96984</td>\n",
              "      <td>34.42438</td>\n",
              "      <td>11.72670</td>\n",
              "      <td>1.36790</td>\n",
              "      <td>7.79444</td>\n",
              "      <td>-0.36994</td>\n",
              "      <td>-133.67852</td>\n",
              "      <td>-83.26165</td>\n",
              "      <td>-37.29765</td>\n",
              "      <td>...</td>\n",
              "      <td>-25.38187</td>\n",
              "      <td>-3.90772</td>\n",
              "      <td>13.29258</td>\n",
              "      <td>41.55060</td>\n",
              "      <td>-7.26272</td>\n",
              "      <td>-21.00863</td>\n",
              "      <td>105.50848</td>\n",
              "      <td>64.29856</td>\n",
              "      <td>26.08481</td>\n",
              "      <td>-44.59110</td>\n",
              "      <td>-8.30657</td>\n",
              "      <td>7.93706</td>\n",
              "      <td>-10.73660</td>\n",
              "      <td>-95.44766</td>\n",
              "      <td>-82.03307</td>\n",
              "      <td>-35.59194</td>\n",
              "      <td>4.69525</td>\n",
              "      <td>70.95626</td>\n",
              "      <td>28.09139</td>\n",
              "      <td>6.02015</td>\n",
              "      <td>-37.13767</td>\n",
              "      <td>-41.12450</td>\n",
              "      <td>-8.40816</td>\n",
              "      <td>7.19877</td>\n",
              "      <td>-8.60176</td>\n",
              "      <td>-5.90857</td>\n",
              "      <td>-12.32437</td>\n",
              "      <td>14.68734</td>\n",
              "      <td>-54.32125</td>\n",
              "      <td>40.14786</td>\n",
              "      <td>13.01620</td>\n",
              "      <td>-54.40548</td>\n",
              "      <td>58.99367</td>\n",
              "      <td>15.37344</td>\n",
              "      <td>1.11144</td>\n",
              "      <td>-23.08793</td>\n",
              "      <td>68.40795</td>\n",
              "      <td>-1.82223</td>\n",
              "      <td>-27.46348</td>\n",
              "      <td>2.26327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2001</td>\n",
              "      <td>48.73215</td>\n",
              "      <td>18.42930</td>\n",
              "      <td>70.32679</td>\n",
              "      <td>12.94636</td>\n",
              "      <td>-10.32437</td>\n",
              "      <td>-24.83777</td>\n",
              "      <td>8.76630</td>\n",
              "      <td>-0.92019</td>\n",
              "      <td>18.76548</td>\n",
              "      <td>4.59210</td>\n",
              "      <td>2.21920</td>\n",
              "      <td>0.34006</td>\n",
              "      <td>44.38997</td>\n",
              "      <td>2056.93836</td>\n",
              "      <td>605.40696</td>\n",
              "      <td>457.41175</td>\n",
              "      <td>777.15347</td>\n",
              "      <td>415.64880</td>\n",
              "      <td>746.47775</td>\n",
              "      <td>366.45320</td>\n",
              "      <td>317.82946</td>\n",
              "      <td>273.07917</td>\n",
              "      <td>141.75921</td>\n",
              "      <td>317.35269</td>\n",
              "      <td>19.48271</td>\n",
              "      <td>-65.25496</td>\n",
              "      <td>162.75145</td>\n",
              "      <td>135.00765</td>\n",
              "      <td>-96.28436</td>\n",
              "      <td>-86.87955</td>\n",
              "      <td>17.38087</td>\n",
              "      <td>45.90742</td>\n",
              "      <td>32.49908</td>\n",
              "      <td>-32.85429</td>\n",
              "      <td>45.10830</td>\n",
              "      <td>26.84939</td>\n",
              "      <td>-302.57328</td>\n",
              "      <td>-41.71932</td>\n",
              "      <td>-138.85034</td>\n",
              "      <td>...</td>\n",
              "      <td>28.55107</td>\n",
              "      <td>1.52298</td>\n",
              "      <td>70.99515</td>\n",
              "      <td>-43.63073</td>\n",
              "      <td>-42.55014</td>\n",
              "      <td>129.82848</td>\n",
              "      <td>79.95420</td>\n",
              "      <td>-87.14554</td>\n",
              "      <td>-45.75446</td>\n",
              "      <td>-65.82100</td>\n",
              "      <td>-43.90031</td>\n",
              "      <td>-19.45705</td>\n",
              "      <td>12.59163</td>\n",
              "      <td>-407.64130</td>\n",
              "      <td>42.91189</td>\n",
              "      <td>12.15850</td>\n",
              "      <td>-88.37882</td>\n",
              "      <td>42.25246</td>\n",
              "      <td>46.49209</td>\n",
              "      <td>-30.17747</td>\n",
              "      <td>45.98495</td>\n",
              "      <td>130.47892</td>\n",
              "      <td>13.88281</td>\n",
              "      <td>-4.00055</td>\n",
              "      <td>17.85965</td>\n",
              "      <td>-18.32138</td>\n",
              "      <td>-87.99109</td>\n",
              "      <td>14.37524</td>\n",
              "      <td>-22.70119</td>\n",
              "      <td>-58.81266</td>\n",
              "      <td>5.66812</td>\n",
              "      <td>-19.68073</td>\n",
              "      <td>33.04964</td>\n",
              "      <td>42.87836</td>\n",
              "      <td>-9.90378</td>\n",
              "      <td>-32.22788</td>\n",
              "      <td>70.49388</td>\n",
              "      <td>12.04941</td>\n",
              "      <td>58.43453</td>\n",
              "      <td>26.92061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2001</td>\n",
              "      <td>50.95714</td>\n",
              "      <td>31.85602</td>\n",
              "      <td>55.81851</td>\n",
              "      <td>13.41693</td>\n",
              "      <td>-6.57898</td>\n",
              "      <td>-18.54940</td>\n",
              "      <td>-3.27872</td>\n",
              "      <td>-2.35035</td>\n",
              "      <td>16.07017</td>\n",
              "      <td>1.39518</td>\n",
              "      <td>2.73553</td>\n",
              "      <td>0.82804</td>\n",
              "      <td>7.46586</td>\n",
              "      <td>699.54544</td>\n",
              "      <td>1016.00954</td>\n",
              "      <td>594.06748</td>\n",
              "      <td>355.73663</td>\n",
              "      <td>507.39931</td>\n",
              "      <td>387.69910</td>\n",
              "      <td>287.15347</td>\n",
              "      <td>112.37152</td>\n",
              "      <td>161.68928</td>\n",
              "      <td>144.14353</td>\n",
              "      <td>199.29693</td>\n",
              "      <td>-4.24359</td>\n",
              "      <td>-297.00587</td>\n",
              "      <td>-148.36392</td>\n",
              "      <td>-7.94726</td>\n",
              "      <td>-18.71630</td>\n",
              "      <td>12.77542</td>\n",
              "      <td>-25.37725</td>\n",
              "      <td>9.71410</td>\n",
              "      <td>0.13843</td>\n",
              "      <td>26.79723</td>\n",
              "      <td>6.30760</td>\n",
              "      <td>28.70107</td>\n",
              "      <td>-74.89005</td>\n",
              "      <td>-289.19553</td>\n",
              "      <td>-166.26089</td>\n",
              "      <td>...</td>\n",
              "      <td>18.50939</td>\n",
              "      <td>16.97216</td>\n",
              "      <td>24.26629</td>\n",
              "      <td>-10.50788</td>\n",
              "      <td>-8.68412</td>\n",
              "      <td>54.75759</td>\n",
              "      <td>194.74034</td>\n",
              "      <td>7.95966</td>\n",
              "      <td>-18.22685</td>\n",
              "      <td>0.06463</td>\n",
              "      <td>-2.63069</td>\n",
              "      <td>26.02561</td>\n",
              "      <td>1.75729</td>\n",
              "      <td>-262.36917</td>\n",
              "      <td>-233.60089</td>\n",
              "      <td>-2.50502</td>\n",
              "      <td>-12.14279</td>\n",
              "      <td>81.37617</td>\n",
              "      <td>2.07554</td>\n",
              "      <td>-1.82381</td>\n",
              "      <td>183.65292</td>\n",
              "      <td>22.64797</td>\n",
              "      <td>-39.98887</td>\n",
              "      <td>43.37381</td>\n",
              "      <td>-31.56737</td>\n",
              "      <td>-4.88840</td>\n",
              "      <td>-36.53213</td>\n",
              "      <td>-23.94662</td>\n",
              "      <td>-84.19275</td>\n",
              "      <td>66.00518</td>\n",
              "      <td>3.03800</td>\n",
              "      <td>26.05866</td>\n",
              "      <td>-50.92779</td>\n",
              "      <td>10.93792</td>\n",
              "      <td>-0.07568</td>\n",
              "      <td>43.20130</td>\n",
              "      <td>-115.00698</td>\n",
              "      <td>-0.05859</td>\n",
              "      <td>39.67068</td>\n",
              "      <td>-0.66345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2001</td>\n",
              "      <td>48.24750</td>\n",
              "      <td>-1.89837</td>\n",
              "      <td>36.29772</td>\n",
              "      <td>2.58776</td>\n",
              "      <td>0.97170</td>\n",
              "      <td>-26.21683</td>\n",
              "      <td>5.05097</td>\n",
              "      <td>-10.34124</td>\n",
              "      <td>3.55005</td>\n",
              "      <td>-6.36304</td>\n",
              "      <td>6.63016</td>\n",
              "      <td>-3.35142</td>\n",
              "      <td>37.64085</td>\n",
              "      <td>2174.08189</td>\n",
              "      <td>697.43346</td>\n",
              "      <td>459.24587</td>\n",
              "      <td>742.78961</td>\n",
              "      <td>229.30783</td>\n",
              "      <td>387.89697</td>\n",
              "      <td>249.06662</td>\n",
              "      <td>245.89870</td>\n",
              "      <td>176.20527</td>\n",
              "      <td>98.82222</td>\n",
              "      <td>150.97286</td>\n",
              "      <td>78.49057</td>\n",
              "      <td>-62.00282</td>\n",
              "      <td>43.49659</td>\n",
              "      <td>-96.42719</td>\n",
              "      <td>-108.96608</td>\n",
              "      <td>14.22854</td>\n",
              "      <td>14.54178</td>\n",
              "      <td>-23.55608</td>\n",
              "      <td>-39.36953</td>\n",
              "      <td>-43.59209</td>\n",
              "      <td>20.83714</td>\n",
              "      <td>35.63919</td>\n",
              "      <td>-181.34947</td>\n",
              "      <td>-93.66614</td>\n",
              "      <td>-90.55616</td>\n",
              "      <td>...</td>\n",
              "      <td>4.56917</td>\n",
              "      <td>-37.32280</td>\n",
              "      <td>4.15159</td>\n",
              "      <td>12.24315</td>\n",
              "      <td>35.02697</td>\n",
              "      <td>-178.89573</td>\n",
              "      <td>82.46573</td>\n",
              "      <td>-20.49425</td>\n",
              "      <td>101.78577</td>\n",
              "      <td>-19.77808</td>\n",
              "      <td>-21.52657</td>\n",
              "      <td>3.36303</td>\n",
              "      <td>-11.63176</td>\n",
              "      <td>51.55411</td>\n",
              "      <td>-50.57576</td>\n",
              "      <td>-28.14755</td>\n",
              "      <td>-83.15795</td>\n",
              "      <td>-7.35260</td>\n",
              "      <td>-22.11505</td>\n",
              "      <td>1.18279</td>\n",
              "      <td>-122.70467</td>\n",
              "      <td>150.57360</td>\n",
              "      <td>24.37468</td>\n",
              "      <td>41.19821</td>\n",
              "      <td>-37.04318</td>\n",
              "      <td>-28.72986</td>\n",
              "      <td>162.19614</td>\n",
              "      <td>22.18309</td>\n",
              "      <td>-8.63509</td>\n",
              "      <td>85.23416</td>\n",
              "      <td>34.57337</td>\n",
              "      <td>-171.70734</td>\n",
              "      <td>-16.96705</td>\n",
              "      <td>-46.67617</td>\n",
              "      <td>-12.51516</td>\n",
              "      <td>82.58061</td>\n",
              "      <td>-72.08993</td>\n",
              "      <td>9.90558</td>\n",
              "      <td>199.62971</td>\n",
              "      <td>18.85382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2001</td>\n",
              "      <td>50.97020</td>\n",
              "      <td>42.20998</td>\n",
              "      <td>67.09964</td>\n",
              "      <td>8.46791</td>\n",
              "      <td>-15.85279</td>\n",
              "      <td>-16.81409</td>\n",
              "      <td>-12.48207</td>\n",
              "      <td>-9.37636</td>\n",
              "      <td>12.63699</td>\n",
              "      <td>0.93609</td>\n",
              "      <td>1.60923</td>\n",
              "      <td>2.19223</td>\n",
              "      <td>47.32082</td>\n",
              "      <td>894.28471</td>\n",
              "      <td>809.86615</td>\n",
              "      <td>318.78559</td>\n",
              "      <td>435.04497</td>\n",
              "      <td>341.61467</td>\n",
              "      <td>334.30734</td>\n",
              "      <td>322.99589</td>\n",
              "      <td>190.61921</td>\n",
              "      <td>235.84715</td>\n",
              "      <td>96.89517</td>\n",
              "      <td>210.58870</td>\n",
              "      <td>5.60463</td>\n",
              "      <td>-199.63958</td>\n",
              "      <td>204.85812</td>\n",
              "      <td>-77.17695</td>\n",
              "      <td>-65.79741</td>\n",
              "      <td>-6.95097</td>\n",
              "      <td>-12.15262</td>\n",
              "      <td>-3.85410</td>\n",
              "      <td>20.68990</td>\n",
              "      <td>-20.30480</td>\n",
              "      <td>37.15045</td>\n",
              "      <td>11.20673</td>\n",
              "      <td>-124.09519</td>\n",
              "      <td>-295.98542</td>\n",
              "      <td>-33.31169</td>\n",
              "      <td>...</td>\n",
              "      <td>45.25506</td>\n",
              "      <td>10.42226</td>\n",
              "      <td>27.88782</td>\n",
              "      <td>-17.12676</td>\n",
              "      <td>-31.54772</td>\n",
              "      <td>-76.86293</td>\n",
              "      <td>41.17343</td>\n",
              "      <td>-138.32535</td>\n",
              "      <td>-53.96905</td>\n",
              "      <td>-21.30266</td>\n",
              "      <td>-24.87362</td>\n",
              "      <td>-2.46595</td>\n",
              "      <td>-4.05003</td>\n",
              "      <td>-56.51161</td>\n",
              "      <td>-34.56445</td>\n",
              "      <td>-5.07092</td>\n",
              "      <td>-47.75605</td>\n",
              "      <td>64.81513</td>\n",
              "      <td>-97.42948</td>\n",
              "      <td>-12.59418</td>\n",
              "      <td>55.23699</td>\n",
              "      <td>28.85657</td>\n",
              "      <td>54.53513</td>\n",
              "      <td>-31.97077</td>\n",
              "      <td>20.03279</td>\n",
              "      <td>-8.07892</td>\n",
              "      <td>-55.12617</td>\n",
              "      <td>26.58961</td>\n",
              "      <td>-10.27183</td>\n",
              "      <td>-30.64232</td>\n",
              "      <td>9.92661</td>\n",
              "      <td>-55.95724</td>\n",
              "      <td>64.92712</td>\n",
              "      <td>-17.72522</td>\n",
              "      <td>-1.49237</td>\n",
              "      <td>-7.50035</td>\n",
              "      <td>51.76631</td>\n",
              "      <td>7.88713</td>\n",
              "      <td>55.66926</td>\n",
              "      <td>28.74903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515340</th>\n",
              "      <td>2006</td>\n",
              "      <td>51.28467</td>\n",
              "      <td>45.88068</td>\n",
              "      <td>22.19582</td>\n",
              "      <td>-5.53319</td>\n",
              "      <td>-3.61835</td>\n",
              "      <td>-16.36914</td>\n",
              "      <td>2.12652</td>\n",
              "      <td>5.18160</td>\n",
              "      <td>-8.66890</td>\n",
              "      <td>2.67217</td>\n",
              "      <td>0.45234</td>\n",
              "      <td>2.51380</td>\n",
              "      <td>18.79583</td>\n",
              "      <td>592.17931</td>\n",
              "      <td>619.01842</td>\n",
              "      <td>681.30323</td>\n",
              "      <td>415.21939</td>\n",
              "      <td>639.90327</td>\n",
              "      <td>287.20710</td>\n",
              "      <td>375.31963</td>\n",
              "      <td>212.76265</td>\n",
              "      <td>246.26651</td>\n",
              "      <td>143.48234</td>\n",
              "      <td>217.45556</td>\n",
              "      <td>9.90577</td>\n",
              "      <td>-62.51153</td>\n",
              "      <td>-76.96635</td>\n",
              "      <td>-60.62065</td>\n",
              "      <td>67.81811</td>\n",
              "      <td>-9.20742</td>\n",
              "      <td>-30.73303</td>\n",
              "      <td>21.58525</td>\n",
              "      <td>-31.21664</td>\n",
              "      <td>-36.39659</td>\n",
              "      <td>28.18814</td>\n",
              "      <td>39.46981</td>\n",
              "      <td>-77.13200</td>\n",
              "      <td>-43.39948</td>\n",
              "      <td>-57.69462</td>\n",
              "      <td>...</td>\n",
              "      <td>-74.40960</td>\n",
              "      <td>78.78128</td>\n",
              "      <td>-14.74786</td>\n",
              "      <td>18.02148</td>\n",
              "      <td>-19.61304</td>\n",
              "      <td>-50.34714</td>\n",
              "      <td>87.06521</td>\n",
              "      <td>43.77874</td>\n",
              "      <td>-5.00339</td>\n",
              "      <td>101.08108</td>\n",
              "      <td>-13.34314</td>\n",
              "      <td>-59.17573</td>\n",
              "      <td>-46.22182</td>\n",
              "      <td>-27.10155</td>\n",
              "      <td>-7.07840</td>\n",
              "      <td>23.04732</td>\n",
              "      <td>29.32027</td>\n",
              "      <td>2.10740</td>\n",
              "      <td>-5.77951</td>\n",
              "      <td>2.68326</td>\n",
              "      <td>-13.78081</td>\n",
              "      <td>6.33542</td>\n",
              "      <td>-37.38191</td>\n",
              "      <td>-14.90918</td>\n",
              "      <td>26.87263</td>\n",
              "      <td>7.07232</td>\n",
              "      <td>-127.04955</td>\n",
              "      <td>86.78200</td>\n",
              "      <td>-68.14511</td>\n",
              "      <td>67.44416</td>\n",
              "      <td>4.81440</td>\n",
              "      <td>-3.75991</td>\n",
              "      <td>-30.92584</td>\n",
              "      <td>26.33968</td>\n",
              "      <td>-5.03390</td>\n",
              "      <td>21.86037</td>\n",
              "      <td>-142.29410</td>\n",
              "      <td>3.42901</td>\n",
              "      <td>-41.14721</td>\n",
              "      <td>-15.46052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515341</th>\n",
              "      <td>2006</td>\n",
              "      <td>49.87870</td>\n",
              "      <td>37.93125</td>\n",
              "      <td>18.65987</td>\n",
              "      <td>-3.63581</td>\n",
              "      <td>-27.75665</td>\n",
              "      <td>-18.52988</td>\n",
              "      <td>7.76108</td>\n",
              "      <td>3.56109</td>\n",
              "      <td>-2.50351</td>\n",
              "      <td>2.20175</td>\n",
              "      <td>-0.58487</td>\n",
              "      <td>-9.78657</td>\n",
              "      <td>35.81410</td>\n",
              "      <td>1047.28364</td>\n",
              "      <td>1451.87226</td>\n",
              "      <td>633.17982</td>\n",
              "      <td>448.46796</td>\n",
              "      <td>826.14418</td>\n",
              "      <td>277.55902</td>\n",
              "      <td>202.20787</td>\n",
              "      <td>241.85866</td>\n",
              "      <td>199.31274</td>\n",
              "      <td>180.60934</td>\n",
              "      <td>168.49980</td>\n",
              "      <td>89.28058</td>\n",
              "      <td>237.30605</td>\n",
              "      <td>-72.22211</td>\n",
              "      <td>-10.02772</td>\n",
              "      <td>-41.24980</td>\n",
              "      <td>-7.59473</td>\n",
              "      <td>-5.23307</td>\n",
              "      <td>24.88978</td>\n",
              "      <td>39.42813</td>\n",
              "      <td>-40.17760</td>\n",
              "      <td>26.51372</td>\n",
              "      <td>79.84191</td>\n",
              "      <td>-15.49724</td>\n",
              "      <td>46.37942</td>\n",
              "      <td>-209.97900</td>\n",
              "      <td>...</td>\n",
              "      <td>-61.06002</td>\n",
              "      <td>50.86072</td>\n",
              "      <td>-3.54799</td>\n",
              "      <td>36.50303</td>\n",
              "      <td>20.94570</td>\n",
              "      <td>-79.43478</td>\n",
              "      <td>-15.49133</td>\n",
              "      <td>17.79165</td>\n",
              "      <td>95.84510</td>\n",
              "      <td>-37.68620</td>\n",
              "      <td>8.51302</td>\n",
              "      <td>13.72492</td>\n",
              "      <td>-71.83419</td>\n",
              "      <td>-191.37407</td>\n",
              "      <td>-34.71662</td>\n",
              "      <td>28.34789</td>\n",
              "      <td>45.25187</td>\n",
              "      <td>17.07862</td>\n",
              "      <td>31.46894</td>\n",
              "      <td>-13.44802</td>\n",
              "      <td>38.68815</td>\n",
              "      <td>109.03046</td>\n",
              "      <td>-42.45525</td>\n",
              "      <td>18.67531</td>\n",
              "      <td>-50.86612</td>\n",
              "      <td>11.26242</td>\n",
              "      <td>59.30165</td>\n",
              "      <td>178.15846</td>\n",
              "      <td>-29.04997</td>\n",
              "      <td>70.22336</td>\n",
              "      <td>32.38589</td>\n",
              "      <td>-32.75535</td>\n",
              "      <td>-61.05473</td>\n",
              "      <td>56.65182</td>\n",
              "      <td>15.29965</td>\n",
              "      <td>95.88193</td>\n",
              "      <td>-10.63242</td>\n",
              "      <td>12.96552</td>\n",
              "      <td>92.11633</td>\n",
              "      <td>10.88815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515342</th>\n",
              "      <td>2006</td>\n",
              "      <td>45.12852</td>\n",
              "      <td>12.65758</td>\n",
              "      <td>-38.72018</td>\n",
              "      <td>8.80882</td>\n",
              "      <td>-29.29985</td>\n",
              "      <td>-2.28706</td>\n",
              "      <td>-18.40424</td>\n",
              "      <td>-22.28726</td>\n",
              "      <td>-4.52429</td>\n",
              "      <td>-11.46411</td>\n",
              "      <td>3.28514</td>\n",
              "      <td>1.99943</td>\n",
              "      <td>27.77109</td>\n",
              "      <td>1693.72442</td>\n",
              "      <td>3825.48305</td>\n",
              "      <td>2714.53243</td>\n",
              "      <td>1036.34216</td>\n",
              "      <td>1171.81248</td>\n",
              "      <td>468.44308</td>\n",
              "      <td>1042.15436</td>\n",
              "      <td>278.94429</td>\n",
              "      <td>497.83085</td>\n",
              "      <td>423.82729</td>\n",
              "      <td>239.91028</td>\n",
              "      <td>-61.01287</td>\n",
              "      <td>-1383.48696</td>\n",
              "      <td>-1828.43740</td>\n",
              "      <td>-131.54731</td>\n",
              "      <td>138.81510</td>\n",
              "      <td>51.36991</td>\n",
              "      <td>-45.25035</td>\n",
              "      <td>138.31791</td>\n",
              "      <td>-107.60348</td>\n",
              "      <td>-17.01878</td>\n",
              "      <td>-36.53276</td>\n",
              "      <td>226.67213</td>\n",
              "      <td>716.76768</td>\n",
              "      <td>-267.06525</td>\n",
              "      <td>-362.27860</td>\n",
              "      <td>...</td>\n",
              "      <td>191.56779</td>\n",
              "      <td>72.49396</td>\n",
              "      <td>-38.96949</td>\n",
              "      <td>61.22195</td>\n",
              "      <td>24.49062</td>\n",
              "      <td>182.62433</td>\n",
              "      <td>510.41684</td>\n",
              "      <td>-379.38804</td>\n",
              "      <td>226.54992</td>\n",
              "      <td>-201.28237</td>\n",
              "      <td>6.89971</td>\n",
              "      <td>86.07237</td>\n",
              "      <td>-42.85773</td>\n",
              "      <td>-215.01900</td>\n",
              "      <td>88.60866</td>\n",
              "      <td>14.51385</td>\n",
              "      <td>-28.33832</td>\n",
              "      <td>255.17385</td>\n",
              "      <td>14.17125</td>\n",
              "      <td>25.06417</td>\n",
              "      <td>218.85618</td>\n",
              "      <td>-222.53173</td>\n",
              "      <td>35.58546</td>\n",
              "      <td>30.88622</td>\n",
              "      <td>-24.91594</td>\n",
              "      <td>-2.65009</td>\n",
              "      <td>-69.53483</td>\n",
              "      <td>333.67598</td>\n",
              "      <td>-28.24399</td>\n",
              "      <td>202.51566</td>\n",
              "      <td>-18.73598</td>\n",
              "      <td>-71.15954</td>\n",
              "      <td>-123.98443</td>\n",
              "      <td>121.26989</td>\n",
              "      <td>10.89629</td>\n",
              "      <td>34.62409</td>\n",
              "      <td>-248.61020</td>\n",
              "      <td>-6.07171</td>\n",
              "      <td>53.96319</td>\n",
              "      <td>-8.09364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515343</th>\n",
              "      <td>2006</td>\n",
              "      <td>44.16614</td>\n",
              "      <td>32.38368</td>\n",
              "      <td>-3.34971</td>\n",
              "      <td>-2.49165</td>\n",
              "      <td>-19.59278</td>\n",
              "      <td>-18.67098</td>\n",
              "      <td>8.78428</td>\n",
              "      <td>4.02039</td>\n",
              "      <td>-12.01230</td>\n",
              "      <td>-0.74075</td>\n",
              "      <td>-1.26523</td>\n",
              "      <td>-4.41983</td>\n",
              "      <td>140.44937</td>\n",
              "      <td>2850.23336</td>\n",
              "      <td>1875.28895</td>\n",
              "      <td>1362.98053</td>\n",
              "      <td>784.39737</td>\n",
              "      <td>908.09838</td>\n",
              "      <td>367.12005</td>\n",
              "      <td>692.58547</td>\n",
              "      <td>286.72625</td>\n",
              "      <td>395.46735</td>\n",
              "      <td>221.19089</td>\n",
              "      <td>211.62098</td>\n",
              "      <td>141.17304</td>\n",
              "      <td>647.52054</td>\n",
              "      <td>-451.67671</td>\n",
              "      <td>-170.33993</td>\n",
              "      <td>-106.30851</td>\n",
              "      <td>129.80285</td>\n",
              "      <td>-118.54997</td>\n",
              "      <td>116.14019</td>\n",
              "      <td>-18.36186</td>\n",
              "      <td>-29.42843</td>\n",
              "      <td>13.59803</td>\n",
              "      <td>296.86552</td>\n",
              "      <td>-332.24640</td>\n",
              "      <td>219.84847</td>\n",
              "      <td>-180.27193</td>\n",
              "      <td>...</td>\n",
              "      <td>14.33401</td>\n",
              "      <td>-10.61959</td>\n",
              "      <td>-37.44137</td>\n",
              "      <td>32.72492</td>\n",
              "      <td>-16.62357</td>\n",
              "      <td>-343.07974</td>\n",
              "      <td>148.00075</td>\n",
              "      <td>-64.73672</td>\n",
              "      <td>59.16029</td>\n",
              "      <td>-129.60142</td>\n",
              "      <td>24.47146</td>\n",
              "      <td>-90.78617</td>\n",
              "      <td>-34.58624</td>\n",
              "      <td>-285.37506</td>\n",
              "      <td>-8.78066</td>\n",
              "      <td>63.91160</td>\n",
              "      <td>58.86067</td>\n",
              "      <td>43.28537</td>\n",
              "      <td>22.69472</td>\n",
              "      <td>-0.93940</td>\n",
              "      <td>417.76862</td>\n",
              "      <td>78.26912</td>\n",
              "      <td>-173.95232</td>\n",
              "      <td>-35.42845</td>\n",
              "      <td>10.59859</td>\n",
              "      <td>-16.51518</td>\n",
              "      <td>157.75671</td>\n",
              "      <td>294.31838</td>\n",
              "      <td>-37.30155</td>\n",
              "      <td>80.00327</td>\n",
              "      <td>67.16763</td>\n",
              "      <td>282.77624</td>\n",
              "      <td>-4.63677</td>\n",
              "      <td>144.00125</td>\n",
              "      <td>21.62652</td>\n",
              "      <td>-29.72432</td>\n",
              "      <td>71.47198</td>\n",
              "      <td>20.32240</td>\n",
              "      <td>14.83107</td>\n",
              "      <td>39.74909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515344</th>\n",
              "      <td>2005</td>\n",
              "      <td>51.85726</td>\n",
              "      <td>59.11655</td>\n",
              "      <td>26.39436</td>\n",
              "      <td>-5.46030</td>\n",
              "      <td>-20.69012</td>\n",
              "      <td>-19.95528</td>\n",
              "      <td>-6.72771</td>\n",
              "      <td>2.29590</td>\n",
              "      <td>10.31018</td>\n",
              "      <td>6.26597</td>\n",
              "      <td>-1.78800</td>\n",
              "      <td>-6.19786</td>\n",
              "      <td>20.16600</td>\n",
              "      <td>598.45275</td>\n",
              "      <td>1140.69539</td>\n",
              "      <td>721.49244</td>\n",
              "      <td>272.84841</td>\n",
              "      <td>564.06690</td>\n",
              "      <td>199.41547</td>\n",
              "      <td>189.04637</td>\n",
              "      <td>217.32042</td>\n",
              "      <td>137.13390</td>\n",
              "      <td>150.34608</td>\n",
              "      <td>98.21589</td>\n",
              "      <td>48.12644</td>\n",
              "      <td>-601.59295</td>\n",
              "      <td>10.58466</td>\n",
              "      <td>-83.35368</td>\n",
              "      <td>96.86756</td>\n",
              "      <td>69.40708</td>\n",
              "      <td>8.06033</td>\n",
              "      <td>-26.01693</td>\n",
              "      <td>-2.93173</td>\n",
              "      <td>26.18398</td>\n",
              "      <td>-12.24660</td>\n",
              "      <td>-14.52391</td>\n",
              "      <td>-121.61676</td>\n",
              "      <td>119.15632</td>\n",
              "      <td>-229.55722</td>\n",
              "      <td>...</td>\n",
              "      <td>1.78072</td>\n",
              "      <td>64.75548</td>\n",
              "      <td>24.55866</td>\n",
              "      <td>-1.12509</td>\n",
              "      <td>-13.58287</td>\n",
              "      <td>-99.66038</td>\n",
              "      <td>-124.73875</td>\n",
              "      <td>67.02630</td>\n",
              "      <td>33.05618</td>\n",
              "      <td>60.25818</td>\n",
              "      <td>28.00288</td>\n",
              "      <td>10.62425</td>\n",
              "      <td>-8.86772</td>\n",
              "      <td>78.13543</td>\n",
              "      <td>-181.10013</td>\n",
              "      <td>74.69489</td>\n",
              "      <td>57.45083</td>\n",
              "      <td>114.08816</td>\n",
              "      <td>-9.91322</td>\n",
              "      <td>7.53612</td>\n",
              "      <td>97.06395</td>\n",
              "      <td>233.17754</td>\n",
              "      <td>-100.68441</td>\n",
              "      <td>27.67012</td>\n",
              "      <td>-37.33008</td>\n",
              "      <td>-0.34676</td>\n",
              "      <td>-207.78766</td>\n",
              "      <td>116.75005</td>\n",
              "      <td>-91.82912</td>\n",
              "      <td>8.35020</td>\n",
              "      <td>-11.50511</td>\n",
              "      <td>-69.18291</td>\n",
              "      <td>60.58456</td>\n",
              "      <td>28.64599</td>\n",
              "      <td>-4.39620</td>\n",
              "      <td>-64.56491</td>\n",
              "      <td>-45.61012</td>\n",
              "      <td>-5.51512</td>\n",
              "      <td>32.35602</td>\n",
              "      <td>12.17352</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>515345 rows Ã— 91 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        year      var1      var2  ...     var88      var89     var90\n",
              "0       2001  49.94357  21.47114  ...  -1.82223  -27.46348   2.26327\n",
              "1       2001  48.73215  18.42930  ...  12.04941   58.43453  26.92061\n",
              "2       2001  50.95714  31.85602  ...  -0.05859   39.67068  -0.66345\n",
              "3       2001  48.24750  -1.89837  ...   9.90558  199.62971  18.85382\n",
              "4       2001  50.97020  42.20998  ...   7.88713   55.66926  28.74903\n",
              "...      ...       ...       ...  ...       ...        ...       ...\n",
              "515340  2006  51.28467  45.88068  ...   3.42901  -41.14721 -15.46052\n",
              "515341  2006  49.87870  37.93125  ...  12.96552   92.11633  10.88815\n",
              "515342  2006  45.12852  12.65758  ...  -6.07171   53.96319  -8.09364\n",
              "515343  2006  44.16614  32.38368  ...  20.32240   14.83107  39.74909\n",
              "515344  2005  51.85726  59.11655  ...  -5.51512   32.35602  12.17352\n",
              "\n",
              "[515345 rows x 91 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaLuAMH_zGrA"
      },
      "source": [
        "To set up our data for classification, we'll use the \"year\" field to represent\n",
        "whether a song was released in the 20-th century. In our case `df[\"year\"]` will be 1 if\n",
        "the year was released after 2000, and 0 otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZdGlNgdzGrA"
      },
      "source": [
        "df[\"year\"] = df[\"year\"].map(lambda x: int(x > 2000))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xugy7FZ8eoAd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "outputId": "3bf5acdb-34fb-417d-96e8-2acbedd1f306"
      },
      "source": [
        "df.head(20)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>var1</th>\n",
              "      <th>var2</th>\n",
              "      <th>var3</th>\n",
              "      <th>var4</th>\n",
              "      <th>var5</th>\n",
              "      <th>var6</th>\n",
              "      <th>var7</th>\n",
              "      <th>var8</th>\n",
              "      <th>var9</th>\n",
              "      <th>var10</th>\n",
              "      <th>var11</th>\n",
              "      <th>var12</th>\n",
              "      <th>var13</th>\n",
              "      <th>var14</th>\n",
              "      <th>var15</th>\n",
              "      <th>var16</th>\n",
              "      <th>var17</th>\n",
              "      <th>var18</th>\n",
              "      <th>var19</th>\n",
              "      <th>var20</th>\n",
              "      <th>var21</th>\n",
              "      <th>var22</th>\n",
              "      <th>var23</th>\n",
              "      <th>var24</th>\n",
              "      <th>var25</th>\n",
              "      <th>var26</th>\n",
              "      <th>var27</th>\n",
              "      <th>var28</th>\n",
              "      <th>var29</th>\n",
              "      <th>var30</th>\n",
              "      <th>var31</th>\n",
              "      <th>var32</th>\n",
              "      <th>var33</th>\n",
              "      <th>var34</th>\n",
              "      <th>var35</th>\n",
              "      <th>var36</th>\n",
              "      <th>var37</th>\n",
              "      <th>var38</th>\n",
              "      <th>var39</th>\n",
              "      <th>...</th>\n",
              "      <th>var51</th>\n",
              "      <th>var52</th>\n",
              "      <th>var53</th>\n",
              "      <th>var54</th>\n",
              "      <th>var55</th>\n",
              "      <th>var56</th>\n",
              "      <th>var57</th>\n",
              "      <th>var58</th>\n",
              "      <th>var59</th>\n",
              "      <th>var60</th>\n",
              "      <th>var61</th>\n",
              "      <th>var62</th>\n",
              "      <th>var63</th>\n",
              "      <th>var64</th>\n",
              "      <th>var65</th>\n",
              "      <th>var66</th>\n",
              "      <th>var67</th>\n",
              "      <th>var68</th>\n",
              "      <th>var69</th>\n",
              "      <th>var70</th>\n",
              "      <th>var71</th>\n",
              "      <th>var72</th>\n",
              "      <th>var73</th>\n",
              "      <th>var74</th>\n",
              "      <th>var75</th>\n",
              "      <th>var76</th>\n",
              "      <th>var77</th>\n",
              "      <th>var78</th>\n",
              "      <th>var79</th>\n",
              "      <th>var80</th>\n",
              "      <th>var81</th>\n",
              "      <th>var82</th>\n",
              "      <th>var83</th>\n",
              "      <th>var84</th>\n",
              "      <th>var85</th>\n",
              "      <th>var86</th>\n",
              "      <th>var87</th>\n",
              "      <th>var88</th>\n",
              "      <th>var89</th>\n",
              "      <th>var90</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>49.94357</td>\n",
              "      <td>21.47114</td>\n",
              "      <td>73.07750</td>\n",
              "      <td>8.74861</td>\n",
              "      <td>-17.40628</td>\n",
              "      <td>-13.09905</td>\n",
              "      <td>-25.01202</td>\n",
              "      <td>-12.23257</td>\n",
              "      <td>7.83089</td>\n",
              "      <td>-2.46783</td>\n",
              "      <td>3.32136</td>\n",
              "      <td>-2.31521</td>\n",
              "      <td>10.20556</td>\n",
              "      <td>611.10913</td>\n",
              "      <td>951.08960</td>\n",
              "      <td>698.11428</td>\n",
              "      <td>408.98485</td>\n",
              "      <td>383.70912</td>\n",
              "      <td>326.51512</td>\n",
              "      <td>238.11327</td>\n",
              "      <td>251.42414</td>\n",
              "      <td>187.17351</td>\n",
              "      <td>100.42652</td>\n",
              "      <td>179.19498</td>\n",
              "      <td>-8.41558</td>\n",
              "      <td>-317.87038</td>\n",
              "      <td>95.86266</td>\n",
              "      <td>48.10259</td>\n",
              "      <td>-95.66303</td>\n",
              "      <td>-18.06215</td>\n",
              "      <td>1.96984</td>\n",
              "      <td>34.42438</td>\n",
              "      <td>11.72670</td>\n",
              "      <td>1.36790</td>\n",
              "      <td>7.79444</td>\n",
              "      <td>-0.36994</td>\n",
              "      <td>-133.67852</td>\n",
              "      <td>-83.26165</td>\n",
              "      <td>-37.29765</td>\n",
              "      <td>...</td>\n",
              "      <td>-25.38187</td>\n",
              "      <td>-3.90772</td>\n",
              "      <td>13.29258</td>\n",
              "      <td>41.55060</td>\n",
              "      <td>-7.26272</td>\n",
              "      <td>-21.00863</td>\n",
              "      <td>105.50848</td>\n",
              "      <td>64.29856</td>\n",
              "      <td>26.08481</td>\n",
              "      <td>-44.59110</td>\n",
              "      <td>-8.30657</td>\n",
              "      <td>7.93706</td>\n",
              "      <td>-10.73660</td>\n",
              "      <td>-95.44766</td>\n",
              "      <td>-82.03307</td>\n",
              "      <td>-35.59194</td>\n",
              "      <td>4.69525</td>\n",
              "      <td>70.95626</td>\n",
              "      <td>28.09139</td>\n",
              "      <td>6.02015</td>\n",
              "      <td>-37.13767</td>\n",
              "      <td>-41.12450</td>\n",
              "      <td>-8.40816</td>\n",
              "      <td>7.19877</td>\n",
              "      <td>-8.60176</td>\n",
              "      <td>-5.90857</td>\n",
              "      <td>-12.32437</td>\n",
              "      <td>14.68734</td>\n",
              "      <td>-54.32125</td>\n",
              "      <td>40.14786</td>\n",
              "      <td>13.01620</td>\n",
              "      <td>-54.40548</td>\n",
              "      <td>58.99367</td>\n",
              "      <td>15.37344</td>\n",
              "      <td>1.11144</td>\n",
              "      <td>-23.08793</td>\n",
              "      <td>68.40795</td>\n",
              "      <td>-1.82223</td>\n",
              "      <td>-27.46348</td>\n",
              "      <td>2.26327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>48.73215</td>\n",
              "      <td>18.42930</td>\n",
              "      <td>70.32679</td>\n",
              "      <td>12.94636</td>\n",
              "      <td>-10.32437</td>\n",
              "      <td>-24.83777</td>\n",
              "      <td>8.76630</td>\n",
              "      <td>-0.92019</td>\n",
              "      <td>18.76548</td>\n",
              "      <td>4.59210</td>\n",
              "      <td>2.21920</td>\n",
              "      <td>0.34006</td>\n",
              "      <td>44.38997</td>\n",
              "      <td>2056.93836</td>\n",
              "      <td>605.40696</td>\n",
              "      <td>457.41175</td>\n",
              "      <td>777.15347</td>\n",
              "      <td>415.64880</td>\n",
              "      <td>746.47775</td>\n",
              "      <td>366.45320</td>\n",
              "      <td>317.82946</td>\n",
              "      <td>273.07917</td>\n",
              "      <td>141.75921</td>\n",
              "      <td>317.35269</td>\n",
              "      <td>19.48271</td>\n",
              "      <td>-65.25496</td>\n",
              "      <td>162.75145</td>\n",
              "      <td>135.00765</td>\n",
              "      <td>-96.28436</td>\n",
              "      <td>-86.87955</td>\n",
              "      <td>17.38087</td>\n",
              "      <td>45.90742</td>\n",
              "      <td>32.49908</td>\n",
              "      <td>-32.85429</td>\n",
              "      <td>45.10830</td>\n",
              "      <td>26.84939</td>\n",
              "      <td>-302.57328</td>\n",
              "      <td>-41.71932</td>\n",
              "      <td>-138.85034</td>\n",
              "      <td>...</td>\n",
              "      <td>28.55107</td>\n",
              "      <td>1.52298</td>\n",
              "      <td>70.99515</td>\n",
              "      <td>-43.63073</td>\n",
              "      <td>-42.55014</td>\n",
              "      <td>129.82848</td>\n",
              "      <td>79.95420</td>\n",
              "      <td>-87.14554</td>\n",
              "      <td>-45.75446</td>\n",
              "      <td>-65.82100</td>\n",
              "      <td>-43.90031</td>\n",
              "      <td>-19.45705</td>\n",
              "      <td>12.59163</td>\n",
              "      <td>-407.64130</td>\n",
              "      <td>42.91189</td>\n",
              "      <td>12.15850</td>\n",
              "      <td>-88.37882</td>\n",
              "      <td>42.25246</td>\n",
              "      <td>46.49209</td>\n",
              "      <td>-30.17747</td>\n",
              "      <td>45.98495</td>\n",
              "      <td>130.47892</td>\n",
              "      <td>13.88281</td>\n",
              "      <td>-4.00055</td>\n",
              "      <td>17.85965</td>\n",
              "      <td>-18.32138</td>\n",
              "      <td>-87.99109</td>\n",
              "      <td>14.37524</td>\n",
              "      <td>-22.70119</td>\n",
              "      <td>-58.81266</td>\n",
              "      <td>5.66812</td>\n",
              "      <td>-19.68073</td>\n",
              "      <td>33.04964</td>\n",
              "      <td>42.87836</td>\n",
              "      <td>-9.90378</td>\n",
              "      <td>-32.22788</td>\n",
              "      <td>70.49388</td>\n",
              "      <td>12.04941</td>\n",
              "      <td>58.43453</td>\n",
              "      <td>26.92061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>50.95714</td>\n",
              "      <td>31.85602</td>\n",
              "      <td>55.81851</td>\n",
              "      <td>13.41693</td>\n",
              "      <td>-6.57898</td>\n",
              "      <td>-18.54940</td>\n",
              "      <td>-3.27872</td>\n",
              "      <td>-2.35035</td>\n",
              "      <td>16.07017</td>\n",
              "      <td>1.39518</td>\n",
              "      <td>2.73553</td>\n",
              "      <td>0.82804</td>\n",
              "      <td>7.46586</td>\n",
              "      <td>699.54544</td>\n",
              "      <td>1016.00954</td>\n",
              "      <td>594.06748</td>\n",
              "      <td>355.73663</td>\n",
              "      <td>507.39931</td>\n",
              "      <td>387.69910</td>\n",
              "      <td>287.15347</td>\n",
              "      <td>112.37152</td>\n",
              "      <td>161.68928</td>\n",
              "      <td>144.14353</td>\n",
              "      <td>199.29693</td>\n",
              "      <td>-4.24359</td>\n",
              "      <td>-297.00587</td>\n",
              "      <td>-148.36392</td>\n",
              "      <td>-7.94726</td>\n",
              "      <td>-18.71630</td>\n",
              "      <td>12.77542</td>\n",
              "      <td>-25.37725</td>\n",
              "      <td>9.71410</td>\n",
              "      <td>0.13843</td>\n",
              "      <td>26.79723</td>\n",
              "      <td>6.30760</td>\n",
              "      <td>28.70107</td>\n",
              "      <td>-74.89005</td>\n",
              "      <td>-289.19553</td>\n",
              "      <td>-166.26089</td>\n",
              "      <td>...</td>\n",
              "      <td>18.50939</td>\n",
              "      <td>16.97216</td>\n",
              "      <td>24.26629</td>\n",
              "      <td>-10.50788</td>\n",
              "      <td>-8.68412</td>\n",
              "      <td>54.75759</td>\n",
              "      <td>194.74034</td>\n",
              "      <td>7.95966</td>\n",
              "      <td>-18.22685</td>\n",
              "      <td>0.06463</td>\n",
              "      <td>-2.63069</td>\n",
              "      <td>26.02561</td>\n",
              "      <td>1.75729</td>\n",
              "      <td>-262.36917</td>\n",
              "      <td>-233.60089</td>\n",
              "      <td>-2.50502</td>\n",
              "      <td>-12.14279</td>\n",
              "      <td>81.37617</td>\n",
              "      <td>2.07554</td>\n",
              "      <td>-1.82381</td>\n",
              "      <td>183.65292</td>\n",
              "      <td>22.64797</td>\n",
              "      <td>-39.98887</td>\n",
              "      <td>43.37381</td>\n",
              "      <td>-31.56737</td>\n",
              "      <td>-4.88840</td>\n",
              "      <td>-36.53213</td>\n",
              "      <td>-23.94662</td>\n",
              "      <td>-84.19275</td>\n",
              "      <td>66.00518</td>\n",
              "      <td>3.03800</td>\n",
              "      <td>26.05866</td>\n",
              "      <td>-50.92779</td>\n",
              "      <td>10.93792</td>\n",
              "      <td>-0.07568</td>\n",
              "      <td>43.20130</td>\n",
              "      <td>-115.00698</td>\n",
              "      <td>-0.05859</td>\n",
              "      <td>39.67068</td>\n",
              "      <td>-0.66345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>48.24750</td>\n",
              "      <td>-1.89837</td>\n",
              "      <td>36.29772</td>\n",
              "      <td>2.58776</td>\n",
              "      <td>0.97170</td>\n",
              "      <td>-26.21683</td>\n",
              "      <td>5.05097</td>\n",
              "      <td>-10.34124</td>\n",
              "      <td>3.55005</td>\n",
              "      <td>-6.36304</td>\n",
              "      <td>6.63016</td>\n",
              "      <td>-3.35142</td>\n",
              "      <td>37.64085</td>\n",
              "      <td>2174.08189</td>\n",
              "      <td>697.43346</td>\n",
              "      <td>459.24587</td>\n",
              "      <td>742.78961</td>\n",
              "      <td>229.30783</td>\n",
              "      <td>387.89697</td>\n",
              "      <td>249.06662</td>\n",
              "      <td>245.89870</td>\n",
              "      <td>176.20527</td>\n",
              "      <td>98.82222</td>\n",
              "      <td>150.97286</td>\n",
              "      <td>78.49057</td>\n",
              "      <td>-62.00282</td>\n",
              "      <td>43.49659</td>\n",
              "      <td>-96.42719</td>\n",
              "      <td>-108.96608</td>\n",
              "      <td>14.22854</td>\n",
              "      <td>14.54178</td>\n",
              "      <td>-23.55608</td>\n",
              "      <td>-39.36953</td>\n",
              "      <td>-43.59209</td>\n",
              "      <td>20.83714</td>\n",
              "      <td>35.63919</td>\n",
              "      <td>-181.34947</td>\n",
              "      <td>-93.66614</td>\n",
              "      <td>-90.55616</td>\n",
              "      <td>...</td>\n",
              "      <td>4.56917</td>\n",
              "      <td>-37.32280</td>\n",
              "      <td>4.15159</td>\n",
              "      <td>12.24315</td>\n",
              "      <td>35.02697</td>\n",
              "      <td>-178.89573</td>\n",
              "      <td>82.46573</td>\n",
              "      <td>-20.49425</td>\n",
              "      <td>101.78577</td>\n",
              "      <td>-19.77808</td>\n",
              "      <td>-21.52657</td>\n",
              "      <td>3.36303</td>\n",
              "      <td>-11.63176</td>\n",
              "      <td>51.55411</td>\n",
              "      <td>-50.57576</td>\n",
              "      <td>-28.14755</td>\n",
              "      <td>-83.15795</td>\n",
              "      <td>-7.35260</td>\n",
              "      <td>-22.11505</td>\n",
              "      <td>1.18279</td>\n",
              "      <td>-122.70467</td>\n",
              "      <td>150.57360</td>\n",
              "      <td>24.37468</td>\n",
              "      <td>41.19821</td>\n",
              "      <td>-37.04318</td>\n",
              "      <td>-28.72986</td>\n",
              "      <td>162.19614</td>\n",
              "      <td>22.18309</td>\n",
              "      <td>-8.63509</td>\n",
              "      <td>85.23416</td>\n",
              "      <td>34.57337</td>\n",
              "      <td>-171.70734</td>\n",
              "      <td>-16.96705</td>\n",
              "      <td>-46.67617</td>\n",
              "      <td>-12.51516</td>\n",
              "      <td>82.58061</td>\n",
              "      <td>-72.08993</td>\n",
              "      <td>9.90558</td>\n",
              "      <td>199.62971</td>\n",
              "      <td>18.85382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>50.97020</td>\n",
              "      <td>42.20998</td>\n",
              "      <td>67.09964</td>\n",
              "      <td>8.46791</td>\n",
              "      <td>-15.85279</td>\n",
              "      <td>-16.81409</td>\n",
              "      <td>-12.48207</td>\n",
              "      <td>-9.37636</td>\n",
              "      <td>12.63699</td>\n",
              "      <td>0.93609</td>\n",
              "      <td>1.60923</td>\n",
              "      <td>2.19223</td>\n",
              "      <td>47.32082</td>\n",
              "      <td>894.28471</td>\n",
              "      <td>809.86615</td>\n",
              "      <td>318.78559</td>\n",
              "      <td>435.04497</td>\n",
              "      <td>341.61467</td>\n",
              "      <td>334.30734</td>\n",
              "      <td>322.99589</td>\n",
              "      <td>190.61921</td>\n",
              "      <td>235.84715</td>\n",
              "      <td>96.89517</td>\n",
              "      <td>210.58870</td>\n",
              "      <td>5.60463</td>\n",
              "      <td>-199.63958</td>\n",
              "      <td>204.85812</td>\n",
              "      <td>-77.17695</td>\n",
              "      <td>-65.79741</td>\n",
              "      <td>-6.95097</td>\n",
              "      <td>-12.15262</td>\n",
              "      <td>-3.85410</td>\n",
              "      <td>20.68990</td>\n",
              "      <td>-20.30480</td>\n",
              "      <td>37.15045</td>\n",
              "      <td>11.20673</td>\n",
              "      <td>-124.09519</td>\n",
              "      <td>-295.98542</td>\n",
              "      <td>-33.31169</td>\n",
              "      <td>...</td>\n",
              "      <td>45.25506</td>\n",
              "      <td>10.42226</td>\n",
              "      <td>27.88782</td>\n",
              "      <td>-17.12676</td>\n",
              "      <td>-31.54772</td>\n",
              "      <td>-76.86293</td>\n",
              "      <td>41.17343</td>\n",
              "      <td>-138.32535</td>\n",
              "      <td>-53.96905</td>\n",
              "      <td>-21.30266</td>\n",
              "      <td>-24.87362</td>\n",
              "      <td>-2.46595</td>\n",
              "      <td>-4.05003</td>\n",
              "      <td>-56.51161</td>\n",
              "      <td>-34.56445</td>\n",
              "      <td>-5.07092</td>\n",
              "      <td>-47.75605</td>\n",
              "      <td>64.81513</td>\n",
              "      <td>-97.42948</td>\n",
              "      <td>-12.59418</td>\n",
              "      <td>55.23699</td>\n",
              "      <td>28.85657</td>\n",
              "      <td>54.53513</td>\n",
              "      <td>-31.97077</td>\n",
              "      <td>20.03279</td>\n",
              "      <td>-8.07892</td>\n",
              "      <td>-55.12617</td>\n",
              "      <td>26.58961</td>\n",
              "      <td>-10.27183</td>\n",
              "      <td>-30.64232</td>\n",
              "      <td>9.92661</td>\n",
              "      <td>-55.95724</td>\n",
              "      <td>64.92712</td>\n",
              "      <td>-17.72522</td>\n",
              "      <td>-1.49237</td>\n",
              "      <td>-7.50035</td>\n",
              "      <td>51.76631</td>\n",
              "      <td>7.88713</td>\n",
              "      <td>55.66926</td>\n",
              "      <td>28.74903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>50.54767</td>\n",
              "      <td>0.31568</td>\n",
              "      <td>92.35066</td>\n",
              "      <td>22.38696</td>\n",
              "      <td>-25.51870</td>\n",
              "      <td>-19.04928</td>\n",
              "      <td>20.67345</td>\n",
              "      <td>-5.19943</td>\n",
              "      <td>3.63566</td>\n",
              "      <td>-4.69088</td>\n",
              "      <td>2.49578</td>\n",
              "      <td>-3.02468</td>\n",
              "      <td>7.69273</td>\n",
              "      <td>1004.95615</td>\n",
              "      <td>785.06709</td>\n",
              "      <td>591.99232</td>\n",
              "      <td>495.75332</td>\n",
              "      <td>291.38165</td>\n",
              "      <td>434.08355</td>\n",
              "      <td>291.55265</td>\n",
              "      <td>303.58860</td>\n",
              "      <td>216.12189</td>\n",
              "      <td>126.10703</td>\n",
              "      <td>147.28090</td>\n",
              "      <td>6.17712</td>\n",
              "      <td>-133.67424</td>\n",
              "      <td>8.69259</td>\n",
              "      <td>-13.76138</td>\n",
              "      <td>-52.72072</td>\n",
              "      <td>-24.00961</td>\n",
              "      <td>-10.82297</td>\n",
              "      <td>11.42039</td>\n",
              "      <td>32.10442</td>\n",
              "      <td>-26.26130</td>\n",
              "      <td>-4.98856</td>\n",
              "      <td>20.84154</td>\n",
              "      <td>-129.69145</td>\n",
              "      <td>-157.91059</td>\n",
              "      <td>-43.29252</td>\n",
              "      <td>...</td>\n",
              "      <td>45.07251</td>\n",
              "      <td>-6.20682</td>\n",
              "      <td>17.53456</td>\n",
              "      <td>43.66313</td>\n",
              "      <td>-4.80024</td>\n",
              "      <td>-33.62226</td>\n",
              "      <td>-15.01731</td>\n",
              "      <td>-33.74416</td>\n",
              "      <td>2.12072</td>\n",
              "      <td>-46.83225</td>\n",
              "      <td>19.99596</td>\n",
              "      <td>26.31683</td>\n",
              "      <td>0.51657</td>\n",
              "      <td>-285.36622</td>\n",
              "      <td>-52.26245</td>\n",
              "      <td>-13.00341</td>\n",
              "      <td>-9.41185</td>\n",
              "      <td>69.69108</td>\n",
              "      <td>33.54907</td>\n",
              "      <td>-11.39665</td>\n",
              "      <td>93.61103</td>\n",
              "      <td>163.39892</td>\n",
              "      <td>5.00362</td>\n",
              "      <td>15.23807</td>\n",
              "      <td>-13.48394</td>\n",
              "      <td>-6.37431</td>\n",
              "      <td>-37.31287</td>\n",
              "      <td>92.47370</td>\n",
              "      <td>-90.00149</td>\n",
              "      <td>47.25143</td>\n",
              "      <td>6.59753</td>\n",
              "      <td>-50.69577</td>\n",
              "      <td>26.02574</td>\n",
              "      <td>18.94430</td>\n",
              "      <td>-0.33730</td>\n",
              "      <td>6.09352</td>\n",
              "      <td>35.18381</td>\n",
              "      <td>5.00283</td>\n",
              "      <td>-11.02257</td>\n",
              "      <td>0.02263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>50.57546</td>\n",
              "      <td>33.17843</td>\n",
              "      <td>50.53517</td>\n",
              "      <td>11.55217</td>\n",
              "      <td>-27.24764</td>\n",
              "      <td>-8.78206</td>\n",
              "      <td>-12.04282</td>\n",
              "      <td>-9.53930</td>\n",
              "      <td>28.61811</td>\n",
              "      <td>8.25435</td>\n",
              "      <td>-0.43743</td>\n",
              "      <td>5.66265</td>\n",
              "      <td>11.07787</td>\n",
              "      <td>1080.98902</td>\n",
              "      <td>1230.78393</td>\n",
              "      <td>1301.63542</td>\n",
              "      <td>952.84686</td>\n",
              "      <td>783.02498</td>\n",
              "      <td>560.79536</td>\n",
              "      <td>696.19620</td>\n",
              "      <td>253.36266</td>\n",
              "      <td>316.92697</td>\n",
              "      <td>151.75689</td>\n",
              "      <td>144.07059</td>\n",
              "      <td>-3.02894</td>\n",
              "      <td>-111.65251</td>\n",
              "      <td>-56.64580</td>\n",
              "      <td>464.86598</td>\n",
              "      <td>150.52166</td>\n",
              "      <td>84.69609</td>\n",
              "      <td>-91.71196</td>\n",
              "      <td>89.31272</td>\n",
              "      <td>16.49867</td>\n",
              "      <td>-4.47074</td>\n",
              "      <td>-2.02539</td>\n",
              "      <td>13.27637</td>\n",
              "      <td>-153.73456</td>\n",
              "      <td>199.01552</td>\n",
              "      <td>-278.79072</td>\n",
              "      <td>...</td>\n",
              "      <td>0.05909</td>\n",
              "      <td>-92.07551</td>\n",
              "      <td>7.80480</td>\n",
              "      <td>-46.15966</td>\n",
              "      <td>-39.03309</td>\n",
              "      <td>32.52065</td>\n",
              "      <td>164.15989</td>\n",
              "      <td>-247.22025</td>\n",
              "      <td>-100.28773</td>\n",
              "      <td>-55.58712</td>\n",
              "      <td>8.38343</td>\n",
              "      <td>-4.57294</td>\n",
              "      <td>-20.08525</td>\n",
              "      <td>-357.00069</td>\n",
              "      <td>-232.78978</td>\n",
              "      <td>-112.81679</td>\n",
              "      <td>-66.16128</td>\n",
              "      <td>43.25003</td>\n",
              "      <td>18.48417</td>\n",
              "      <td>-2.50274</td>\n",
              "      <td>3.25927</td>\n",
              "      <td>94.57509</td>\n",
              "      <td>-24.31254</td>\n",
              "      <td>62.97582</td>\n",
              "      <td>-19.41809</td>\n",
              "      <td>10.35282</td>\n",
              "      <td>-91.89392</td>\n",
              "      <td>10.51922</td>\n",
              "      <td>-74.98521</td>\n",
              "      <td>12.29948</td>\n",
              "      <td>11.63681</td>\n",
              "      <td>25.44182</td>\n",
              "      <td>134.62382</td>\n",
              "      <td>21.51982</td>\n",
              "      <td>8.17570</td>\n",
              "      <td>35.46251</td>\n",
              "      <td>11.57736</td>\n",
              "      <td>4.50056</td>\n",
              "      <td>-4.62739</td>\n",
              "      <td>1.40192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>48.26892</td>\n",
              "      <td>8.97526</td>\n",
              "      <td>75.23158</td>\n",
              "      <td>24.04945</td>\n",
              "      <td>-16.02105</td>\n",
              "      <td>-14.09491</td>\n",
              "      <td>8.11871</td>\n",
              "      <td>-1.87566</td>\n",
              "      <td>7.46701</td>\n",
              "      <td>1.18189</td>\n",
              "      <td>1.46625</td>\n",
              "      <td>-6.34226</td>\n",
              "      <td>8.40470</td>\n",
              "      <td>1619.74629</td>\n",
              "      <td>1762.43083</td>\n",
              "      <td>714.83843</td>\n",
              "      <td>1115.90792</td>\n",
              "      <td>525.85703</td>\n",
              "      <td>576.66300</td>\n",
              "      <td>350.41961</td>\n",
              "      <td>315.44672</td>\n",
              "      <td>267.94363</td>\n",
              "      <td>198.28158</td>\n",
              "      <td>201.97524</td>\n",
              "      <td>17.14701</td>\n",
              "      <td>-517.47978</td>\n",
              "      <td>-11.33396</td>\n",
              "      <td>59.52820</td>\n",
              "      <td>-25.10400</td>\n",
              "      <td>23.48782</td>\n",
              "      <td>12.63888</td>\n",
              "      <td>40.85580</td>\n",
              "      <td>-3.72243</td>\n",
              "      <td>-16.97392</td>\n",
              "      <td>0.04284</td>\n",
              "      <td>59.45956</td>\n",
              "      <td>-191.92216</td>\n",
              "      <td>445.36223</td>\n",
              "      <td>-120.62667</td>\n",
              "      <td>...</td>\n",
              "      <td>63.71442</td>\n",
              "      <td>3.70732</td>\n",
              "      <td>-9.36662</td>\n",
              "      <td>-25.75461</td>\n",
              "      <td>35.16677</td>\n",
              "      <td>-33.17382</td>\n",
              "      <td>13.80469</td>\n",
              "      <td>-107.14403</td>\n",
              "      <td>140.04250</td>\n",
              "      <td>-124.16153</td>\n",
              "      <td>-18.33032</td>\n",
              "      <td>-9.99397</td>\n",
              "      <td>8.96011</td>\n",
              "      <td>-411.27991</td>\n",
              "      <td>-99.75061</td>\n",
              "      <td>-75.51735</td>\n",
              "      <td>-88.57128</td>\n",
              "      <td>22.90222</td>\n",
              "      <td>4.14618</td>\n",
              "      <td>-16.83238</td>\n",
              "      <td>142.14168</td>\n",
              "      <td>326.91932</td>\n",
              "      <td>-3.49405</td>\n",
              "      <td>7.58991</td>\n",
              "      <td>-32.60725</td>\n",
              "      <td>-4.44469</td>\n",
              "      <td>-56.48952</td>\n",
              "      <td>-31.19491</td>\n",
              "      <td>-32.75384</td>\n",
              "      <td>-52.97111</td>\n",
              "      <td>18.03989</td>\n",
              "      <td>-58.46192</td>\n",
              "      <td>-65.56438</td>\n",
              "      <td>46.99856</td>\n",
              "      <td>-4.09602</td>\n",
              "      <td>56.37650</td>\n",
              "      <td>-18.29975</td>\n",
              "      <td>-0.30633</td>\n",
              "      <td>3.98364</td>\n",
              "      <td>-3.72556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>49.75468</td>\n",
              "      <td>33.99581</td>\n",
              "      <td>56.73846</td>\n",
              "      <td>2.89581</td>\n",
              "      <td>-2.92429</td>\n",
              "      <td>-26.44413</td>\n",
              "      <td>1.71392</td>\n",
              "      <td>-0.55644</td>\n",
              "      <td>22.08594</td>\n",
              "      <td>7.43847</td>\n",
              "      <td>-0.03578</td>\n",
              "      <td>1.66534</td>\n",
              "      <td>46.02060</td>\n",
              "      <td>789.58109</td>\n",
              "      <td>607.78514</td>\n",
              "      <td>248.54603</td>\n",
              "      <td>288.44004</td>\n",
              "      <td>330.16459</td>\n",
              "      <td>356.34405</td>\n",
              "      <td>194.21792</td>\n",
              "      <td>194.68297</td>\n",
              "      <td>158.43568</td>\n",
              "      <td>104.05610</td>\n",
              "      <td>183.10695</td>\n",
              "      <td>-39.54223</td>\n",
              "      <td>-225.39832</td>\n",
              "      <td>78.57149</td>\n",
              "      <td>20.62209</td>\n",
              "      <td>-9.44189</td>\n",
              "      <td>15.94754</td>\n",
              "      <td>51.24174</td>\n",
              "      <td>-4.36054</td>\n",
              "      <td>17.60516</td>\n",
              "      <td>9.03638</td>\n",
              "      <td>16.69044</td>\n",
              "      <td>42.70944</td>\n",
              "      <td>-77.62818</td>\n",
              "      <td>46.71905</td>\n",
              "      <td>-109.71028</td>\n",
              "      <td>...</td>\n",
              "      <td>36.17475</td>\n",
              "      <td>3.44547</td>\n",
              "      <td>46.40983</td>\n",
              "      <td>-21.52684</td>\n",
              "      <td>-10.50872</td>\n",
              "      <td>-8.62787</td>\n",
              "      <td>55.23069</td>\n",
              "      <td>-41.74452</td>\n",
              "      <td>-20.24522</td>\n",
              "      <td>21.78558</td>\n",
              "      <td>-8.69348</td>\n",
              "      <td>-1.85679</td>\n",
              "      <td>3.60117</td>\n",
              "      <td>-53.94252</td>\n",
              "      <td>-54.66970</td>\n",
              "      <td>-15.14957</td>\n",
              "      <td>-46.61675</td>\n",
              "      <td>48.45009</td>\n",
              "      <td>2.77737</td>\n",
              "      <td>-7.80854</td>\n",
              "      <td>38.29390</td>\n",
              "      <td>120.35575</td>\n",
              "      <td>23.84978</td>\n",
              "      <td>14.87696</td>\n",
              "      <td>-41.76961</td>\n",
              "      <td>-21.05519</td>\n",
              "      <td>-45.79645</td>\n",
              "      <td>9.16222</td>\n",
              "      <td>-21.48052</td>\n",
              "      <td>-9.70822</td>\n",
              "      <td>18.70812</td>\n",
              "      <td>5.20391</td>\n",
              "      <td>-27.75192</td>\n",
              "      <td>17.22100</td>\n",
              "      <td>-0.85210</td>\n",
              "      <td>-15.67150</td>\n",
              "      <td>-26.36257</td>\n",
              "      <td>5.48708</td>\n",
              "      <td>-9.13495</td>\n",
              "      <td>6.08680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>45.17809</td>\n",
              "      <td>46.34234</td>\n",
              "      <td>-40.65357</td>\n",
              "      <td>-2.47909</td>\n",
              "      <td>1.21253</td>\n",
              "      <td>-0.65302</td>\n",
              "      <td>-6.95536</td>\n",
              "      <td>-12.20040</td>\n",
              "      <td>17.02512</td>\n",
              "      <td>2.00002</td>\n",
              "      <td>-1.87785</td>\n",
              "      <td>9.85499</td>\n",
              "      <td>25.59837</td>\n",
              "      <td>1905.18577</td>\n",
              "      <td>3676.09074</td>\n",
              "      <td>1976.85531</td>\n",
              "      <td>913.11216</td>\n",
              "      <td>1957.52415</td>\n",
              "      <td>955.98525</td>\n",
              "      <td>942.72667</td>\n",
              "      <td>439.85991</td>\n",
              "      <td>591.66138</td>\n",
              "      <td>493.40770</td>\n",
              "      <td>496.38516</td>\n",
              "      <td>33.94285</td>\n",
              "      <td>-255.90134</td>\n",
              "      <td>-762.28079</td>\n",
              "      <td>-66.10935</td>\n",
              "      <td>-128.02217</td>\n",
              "      <td>198.12908</td>\n",
              "      <td>-34.44957</td>\n",
              "      <td>176.00397</td>\n",
              "      <td>-140.80069</td>\n",
              "      <td>-22.56380</td>\n",
              "      <td>12.77945</td>\n",
              "      <td>193.30164</td>\n",
              "      <td>314.20949</td>\n",
              "      <td>576.29519</td>\n",
              "      <td>-429.58643</td>\n",
              "      <td>...</td>\n",
              "      <td>-17.48938</td>\n",
              "      <td>75.58779</td>\n",
              "      <td>93.29243</td>\n",
              "      <td>85.83507</td>\n",
              "      <td>47.13972</td>\n",
              "      <td>312.85482</td>\n",
              "      <td>135.50478</td>\n",
              "      <td>-32.47886</td>\n",
              "      <td>49.67063</td>\n",
              "      <td>-214.73180</td>\n",
              "      <td>-77.83503</td>\n",
              "      <td>-47.26902</td>\n",
              "      <td>7.58366</td>\n",
              "      <td>-352.56581</td>\n",
              "      <td>-36.15655</td>\n",
              "      <td>-53.39933</td>\n",
              "      <td>-98.60417</td>\n",
              "      <td>-82.37799</td>\n",
              "      <td>45.81588</td>\n",
              "      <td>-16.91676</td>\n",
              "      <td>18.35888</td>\n",
              "      <td>-315.68965</td>\n",
              "      <td>-3.14554</td>\n",
              "      <td>125.45269</td>\n",
              "      <td>-130.18808</td>\n",
              "      <td>-3.06337</td>\n",
              "      <td>42.26602</td>\n",
              "      <td>-9.04929</td>\n",
              "      <td>26.41570</td>\n",
              "      <td>23.36165</td>\n",
              "      <td>-4.36742</td>\n",
              "      <td>-87.55285</td>\n",
              "      <td>-70.79677</td>\n",
              "      <td>76.57355</td>\n",
              "      <td>-7.71727</td>\n",
              "      <td>3.26926</td>\n",
              "      <td>-298.49845</td>\n",
              "      <td>11.49326</td>\n",
              "      <td>-89.21804</td>\n",
              "      <td>-15.09719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1</td>\n",
              "      <td>39.13076</td>\n",
              "      <td>-23.01763</td>\n",
              "      <td>-36.20583</td>\n",
              "      <td>1.67519</td>\n",
              "      <td>-4.27101</td>\n",
              "      <td>13.01158</td>\n",
              "      <td>8.05718</td>\n",
              "      <td>-8.41088</td>\n",
              "      <td>6.27370</td>\n",
              "      <td>-7.81564</td>\n",
              "      <td>-12.29472</td>\n",
              "      <td>-12.26186</td>\n",
              "      <td>45.05025</td>\n",
              "      <td>2022.36720</td>\n",
              "      <td>3860.79421</td>\n",
              "      <td>3186.91395</td>\n",
              "      <td>849.65196</td>\n",
              "      <td>2835.16969</td>\n",
              "      <td>847.01876</td>\n",
              "      <td>1001.27685</td>\n",
              "      <td>347.33791</td>\n",
              "      <td>500.59470</td>\n",
              "      <td>536.16777</td>\n",
              "      <td>318.73821</td>\n",
              "      <td>50.13600</td>\n",
              "      <td>451.14001</td>\n",
              "      <td>-589.91962</td>\n",
              "      <td>-95.42003</td>\n",
              "      <td>-99.51192</td>\n",
              "      <td>-28.43500</td>\n",
              "      <td>22.34786</td>\n",
              "      <td>132.68163</td>\n",
              "      <td>-111.84550</td>\n",
              "      <td>106.94141</td>\n",
              "      <td>-9.94676</td>\n",
              "      <td>269.57001</td>\n",
              "      <td>416.04429</td>\n",
              "      <td>3.08594</td>\n",
              "      <td>-312.96128</td>\n",
              "      <td>...</td>\n",
              "      <td>364.86126</td>\n",
              "      <td>4.89096</td>\n",
              "      <td>-110.07105</td>\n",
              "      <td>19.43169</td>\n",
              "      <td>-13.69554</td>\n",
              "      <td>464.30985</td>\n",
              "      <td>203.55456</td>\n",
              "      <td>-450.84974</td>\n",
              "      <td>9.54696</td>\n",
              "      <td>-433.98556</td>\n",
              "      <td>40.33083</td>\n",
              "      <td>60.29120</td>\n",
              "      <td>66.28236</td>\n",
              "      <td>-398.01020</td>\n",
              "      <td>582.88308</td>\n",
              "      <td>-93.75781</td>\n",
              "      <td>-31.61288</td>\n",
              "      <td>-311.99559</td>\n",
              "      <td>106.91372</td>\n",
              "      <td>-8.56906</td>\n",
              "      <td>-42.26640</td>\n",
              "      <td>269.83186</td>\n",
              "      <td>10.39815</td>\n",
              "      <td>-14.56199</td>\n",
              "      <td>36.15556</td>\n",
              "      <td>51.77120</td>\n",
              "      <td>23.70949</td>\n",
              "      <td>-15.79585</td>\n",
              "      <td>216.13271</td>\n",
              "      <td>100.04021</td>\n",
              "      <td>32.86051</td>\n",
              "      <td>-26.08461</td>\n",
              "      <td>-186.82429</td>\n",
              "      <td>113.58176</td>\n",
              "      <td>9.28727</td>\n",
              "      <td>44.60282</td>\n",
              "      <td>158.00425</td>\n",
              "      <td>-2.59543</td>\n",
              "      <td>109.19723</td>\n",
              "      <td>23.36143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1</td>\n",
              "      <td>37.66498</td>\n",
              "      <td>-34.05910</td>\n",
              "      <td>-17.36060</td>\n",
              "      <td>-26.77781</td>\n",
              "      <td>-39.95119</td>\n",
              "      <td>-20.75000</td>\n",
              "      <td>-0.10231</td>\n",
              "      <td>-0.89972</td>\n",
              "      <td>-1.30205</td>\n",
              "      <td>-0.93041</td>\n",
              "      <td>-3.30157</td>\n",
              "      <td>-2.37522</td>\n",
              "      <td>35.42988</td>\n",
              "      <td>1166.91594</td>\n",
              "      <td>1373.01959</td>\n",
              "      <td>409.04286</td>\n",
              "      <td>609.84543</td>\n",
              "      <td>271.33121</td>\n",
              "      <td>392.43089</td>\n",
              "      <td>180.98525</td>\n",
              "      <td>215.37174</td>\n",
              "      <td>137.27208</td>\n",
              "      <td>69.71585</td>\n",
              "      <td>187.80590</td>\n",
              "      <td>-12.67805</td>\n",
              "      <td>-347.02236</td>\n",
              "      <td>271.33219</td>\n",
              "      <td>40.08570</td>\n",
              "      <td>42.13962</td>\n",
              "      <td>10.70729</td>\n",
              "      <td>36.75105</td>\n",
              "      <td>6.28685</td>\n",
              "      <td>3.24624</td>\n",
              "      <td>-3.85517</td>\n",
              "      <td>10.67336</td>\n",
              "      <td>-9.47190</td>\n",
              "      <td>-62.45325</td>\n",
              "      <td>212.55872</td>\n",
              "      <td>-33.45944</td>\n",
              "      <td>...</td>\n",
              "      <td>14.26269</td>\n",
              "      <td>12.13975</td>\n",
              "      <td>17.85889</td>\n",
              "      <td>16.82677</td>\n",
              "      <td>-73.67298</td>\n",
              "      <td>30.33815</td>\n",
              "      <td>-101.70001</td>\n",
              "      <td>-46.13208</td>\n",
              "      <td>14.26704</td>\n",
              "      <td>4.31515</td>\n",
              "      <td>-25.67264</td>\n",
              "      <td>8.70269</td>\n",
              "      <td>11.69122</td>\n",
              "      <td>-59.19425</td>\n",
              "      <td>-43.59348</td>\n",
              "      <td>-81.06458</td>\n",
              "      <td>-8.27943</td>\n",
              "      <td>-16.76813</td>\n",
              "      <td>0.83299</td>\n",
              "      <td>47.83962</td>\n",
              "      <td>-50.41094</td>\n",
              "      <td>-146.83861</td>\n",
              "      <td>37.05135</td>\n",
              "      <td>9.58587</td>\n",
              "      <td>11.61329</td>\n",
              "      <td>-11.13671</td>\n",
              "      <td>86.66011</td>\n",
              "      <td>-21.11909</td>\n",
              "      <td>-4.36777</td>\n",
              "      <td>44.53390</td>\n",
              "      <td>11.18909</td>\n",
              "      <td>45.20614</td>\n",
              "      <td>53.83925</td>\n",
              "      <td>2.59467</td>\n",
              "      <td>-4.00958</td>\n",
              "      <td>-47.74886</td>\n",
              "      <td>-170.92864</td>\n",
              "      <td>-5.19009</td>\n",
              "      <td>8.83617</td>\n",
              "      <td>-7.16056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "      <td>26.51957</td>\n",
              "      <td>-148.15762</td>\n",
              "      <td>-13.30095</td>\n",
              "      <td>-7.25851</td>\n",
              "      <td>17.22029</td>\n",
              "      <td>-21.99439</td>\n",
              "      <td>5.51947</td>\n",
              "      <td>3.48418</td>\n",
              "      <td>2.61738</td>\n",
              "      <td>2.51194</td>\n",
              "      <td>-0.53980</td>\n",
              "      <td>4.94747</td>\n",
              "      <td>55.79019</td>\n",
              "      <td>5492.46406</td>\n",
              "      <td>1704.58482</td>\n",
              "      <td>953.13106</td>\n",
              "      <td>959.38535</td>\n",
              "      <td>788.92858</td>\n",
              "      <td>365.45762</td>\n",
              "      <td>270.37477</td>\n",
              "      <td>524.61443</td>\n",
              "      <td>194.81728</td>\n",
              "      <td>139.44778</td>\n",
              "      <td>375.02904</td>\n",
              "      <td>-434.18365</td>\n",
              "      <td>-627.01684</td>\n",
              "      <td>642.48227</td>\n",
              "      <td>-440.32465</td>\n",
              "      <td>-319.81435</td>\n",
              "      <td>10.13236</td>\n",
              "      <td>38.64956</td>\n",
              "      <td>-30.14092</td>\n",
              "      <td>75.22352</td>\n",
              "      <td>-48.15860</td>\n",
              "      <td>80.88233</td>\n",
              "      <td>183.14023</td>\n",
              "      <td>-178.08073</td>\n",
              "      <td>-594.06429</td>\n",
              "      <td>185.11781</td>\n",
              "      <td>...</td>\n",
              "      <td>137.58951</td>\n",
              "      <td>38.38606</td>\n",
              "      <td>-3.31309</td>\n",
              "      <td>5.48498</td>\n",
              "      <td>-128.84776</td>\n",
              "      <td>-303.86613</td>\n",
              "      <td>188.82297</td>\n",
              "      <td>-108.38722</td>\n",
              "      <td>-69.68996</td>\n",
              "      <td>84.70685</td>\n",
              "      <td>-54.19731</td>\n",
              "      <td>-112.09276</td>\n",
              "      <td>85.47803</td>\n",
              "      <td>38.20818</td>\n",
              "      <td>-277.12066</td>\n",
              "      <td>132.27890</td>\n",
              "      <td>91.74732</td>\n",
              "      <td>-11.84666</td>\n",
              "      <td>-28.23002</td>\n",
              "      <td>-21.30369</td>\n",
              "      <td>205.87882</td>\n",
              "      <td>-22.13147</td>\n",
              "      <td>-52.71243</td>\n",
              "      <td>-122.98466</td>\n",
              "      <td>-0.77068</td>\n",
              "      <td>-56.79478</td>\n",
              "      <td>91.28164</td>\n",
              "      <td>-2.31187</td>\n",
              "      <td>49.34151</td>\n",
              "      <td>-243.95844</td>\n",
              "      <td>23.80442</td>\n",
              "      <td>251.76360</td>\n",
              "      <td>18.81642</td>\n",
              "      <td>157.09656</td>\n",
              "      <td>-27.79449</td>\n",
              "      <td>-137.72740</td>\n",
              "      <td>115.28414</td>\n",
              "      <td>23.00230</td>\n",
              "      <td>-164.02536</td>\n",
              "      <td>51.54138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1</td>\n",
              "      <td>37.68491</td>\n",
              "      <td>-26.84185</td>\n",
              "      <td>-27.10566</td>\n",
              "      <td>-14.95883</td>\n",
              "      <td>-5.87200</td>\n",
              "      <td>-21.68979</td>\n",
              "      <td>4.87374</td>\n",
              "      <td>-18.01800</td>\n",
              "      <td>1.52141</td>\n",
              "      <td>-6.81668</td>\n",
              "      <td>6.80117</td>\n",
              "      <td>21.17530</td>\n",
              "      <td>50.35368</td>\n",
              "      <td>6431.63159</td>\n",
              "      <td>3755.88724</td>\n",
              "      <td>850.19745</td>\n",
              "      <td>2540.53952</td>\n",
              "      <td>400.64901</td>\n",
              "      <td>614.21009</td>\n",
              "      <td>324.55934</td>\n",
              "      <td>395.25844</td>\n",
              "      <td>241.66513</td>\n",
              "      <td>99.66852</td>\n",
              "      <td>489.65963</td>\n",
              "      <td>-466.60181</td>\n",
              "      <td>4015.57689</td>\n",
              "      <td>1042.51072</td>\n",
              "      <td>787.79078</td>\n",
              "      <td>-240.53556</td>\n",
              "      <td>-82.21473</td>\n",
              "      <td>92.62933</td>\n",
              "      <td>25.79866</td>\n",
              "      <td>47.82510</td>\n",
              "      <td>-24.31114</td>\n",
              "      <td>39.59818</td>\n",
              "      <td>-318.96984</td>\n",
              "      <td>1174.17699</td>\n",
              "      <td>2574.00913</td>\n",
              "      <td>-116.86438</td>\n",
              "      <td>...</td>\n",
              "      <td>-10.44655</td>\n",
              "      <td>-86.37776</td>\n",
              "      <td>-2.66989</td>\n",
              "      <td>-150.73054</td>\n",
              "      <td>-309.01020</td>\n",
              "      <td>-263.88605</td>\n",
              "      <td>-511.63129</td>\n",
              "      <td>-331.45866</td>\n",
              "      <td>524.10757</td>\n",
              "      <td>-103.81583</td>\n",
              "      <td>-11.11616</td>\n",
              "      <td>146.64710</td>\n",
              "      <td>47.03819</td>\n",
              "      <td>-555.49233</td>\n",
              "      <td>-312.62275</td>\n",
              "      <td>17.44093</td>\n",
              "      <td>114.07544</td>\n",
              "      <td>30.33500</td>\n",
              "      <td>233.32721</td>\n",
              "      <td>36.70535</td>\n",
              "      <td>-449.22124</td>\n",
              "      <td>507.86020</td>\n",
              "      <td>179.49985</td>\n",
              "      <td>-62.12860</td>\n",
              "      <td>20.81610</td>\n",
              "      <td>37.22555</td>\n",
              "      <td>821.44873</td>\n",
              "      <td>179.59958</td>\n",
              "      <td>-37.21579</td>\n",
              "      <td>-632.61945</td>\n",
              "      <td>-67.57637</td>\n",
              "      <td>234.27192</td>\n",
              "      <td>-72.34557</td>\n",
              "      <td>-362.25101</td>\n",
              "      <td>-25.55019</td>\n",
              "      <td>-89.08971</td>\n",
              "      <td>-891.58937</td>\n",
              "      <td>14.11648</td>\n",
              "      <td>-1030.99180</td>\n",
              "      <td>99.28967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>39.11695</td>\n",
              "      <td>-8.29767</td>\n",
              "      <td>-51.37966</td>\n",
              "      <td>-4.42668</td>\n",
              "      <td>-30.06506</td>\n",
              "      <td>-11.95916</td>\n",
              "      <td>-0.85322</td>\n",
              "      <td>-8.86179</td>\n",
              "      <td>11.36680</td>\n",
              "      <td>3.78199</td>\n",
              "      <td>1.54568</td>\n",
              "      <td>0.58662</td>\n",
              "      <td>28.49070</td>\n",
              "      <td>7965.81458</td>\n",
              "      <td>1411.06042</td>\n",
              "      <td>1218.84885</td>\n",
              "      <td>645.93064</td>\n",
              "      <td>1138.49365</td>\n",
              "      <td>552.35786</td>\n",
              "      <td>652.69173</td>\n",
              "      <td>281.26048</td>\n",
              "      <td>396.15845</td>\n",
              "      <td>231.01221</td>\n",
              "      <td>215.80169</td>\n",
              "      <td>345.50690</td>\n",
              "      <td>-1163.24259</td>\n",
              "      <td>-117.88194</td>\n",
              "      <td>162.42114</td>\n",
              "      <td>339.56477</td>\n",
              "      <td>32.54661</td>\n",
              "      <td>-78.76608</td>\n",
              "      <td>9.22956</td>\n",
              "      <td>-7.94960</td>\n",
              "      <td>-15.24124</td>\n",
              "      <td>29.36270</td>\n",
              "      <td>-19.91118</td>\n",
              "      <td>530.29303</td>\n",
              "      <td>284.81573</td>\n",
              "      <td>2.50910</td>\n",
              "      <td>...</td>\n",
              "      <td>50.89571</td>\n",
              "      <td>18.59393</td>\n",
              "      <td>53.83297</td>\n",
              "      <td>-89.69260</td>\n",
              "      <td>-31.45442</td>\n",
              "      <td>188.56576</td>\n",
              "      <td>70.85815</td>\n",
              "      <td>-259.86825</td>\n",
              "      <td>65.62430</td>\n",
              "      <td>-208.96960</td>\n",
              "      <td>-50.55450</td>\n",
              "      <td>1.06368</td>\n",
              "      <td>0.31427</td>\n",
              "      <td>396.93348</td>\n",
              "      <td>105.80580</td>\n",
              "      <td>72.06757</td>\n",
              "      <td>-160.48858</td>\n",
              "      <td>-97.33616</td>\n",
              "      <td>-69.36746</td>\n",
              "      <td>37.07706</td>\n",
              "      <td>-113.27781</td>\n",
              "      <td>-95.78902</td>\n",
              "      <td>155.13377</td>\n",
              "      <td>-22.10761</td>\n",
              "      <td>-1.08949</td>\n",
              "      <td>13.36146</td>\n",
              "      <td>833.00787</td>\n",
              "      <td>-88.78803</td>\n",
              "      <td>30.70543</td>\n",
              "      <td>-63.16836</td>\n",
              "      <td>42.22923</td>\n",
              "      <td>478.26580</td>\n",
              "      <td>-10.33823</td>\n",
              "      <td>-103.76858</td>\n",
              "      <td>39.19511</td>\n",
              "      <td>-98.76636</td>\n",
              "      <td>-122.81061</td>\n",
              "      <td>-2.14942</td>\n",
              "      <td>-211.48202</td>\n",
              "      <td>-12.81569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1</td>\n",
              "      <td>35.05129</td>\n",
              "      <td>-67.97714</td>\n",
              "      <td>-14.20239</td>\n",
              "      <td>-6.68696</td>\n",
              "      <td>-0.61230</td>\n",
              "      <td>-18.70341</td>\n",
              "      <td>-1.31928</td>\n",
              "      <td>-9.46370</td>\n",
              "      <td>5.53492</td>\n",
              "      <td>2.79989</td>\n",
              "      <td>3.34150</td>\n",
              "      <td>18.01445</td>\n",
              "      <td>48.14418</td>\n",
              "      <td>1938.39116</td>\n",
              "      <td>755.44238</td>\n",
              "      <td>1350.57299</td>\n",
              "      <td>793.09536</td>\n",
              "      <td>793.43819</td>\n",
              "      <td>585.72478</td>\n",
              "      <td>280.00734</td>\n",
              "      <td>260.63960</td>\n",
              "      <td>168.98085</td>\n",
              "      <td>148.49783</td>\n",
              "      <td>539.22389</td>\n",
              "      <td>-99.68273</td>\n",
              "      <td>198.55553</td>\n",
              "      <td>332.84941</td>\n",
              "      <td>-139.15915</td>\n",
              "      <td>-166.48695</td>\n",
              "      <td>-186.65004</td>\n",
              "      <td>143.44962</td>\n",
              "      <td>-22.08680</td>\n",
              "      <td>37.34160</td>\n",
              "      <td>-30.72572</td>\n",
              "      <td>64.79269</td>\n",
              "      <td>57.94387</td>\n",
              "      <td>332.16642</td>\n",
              "      <td>77.96230</td>\n",
              "      <td>312.90695</td>\n",
              "      <td>...</td>\n",
              "      <td>52.35627</td>\n",
              "      <td>13.93583</td>\n",
              "      <td>-2.03342</td>\n",
              "      <td>-107.12775</td>\n",
              "      <td>-6.42999</td>\n",
              "      <td>215.24319</td>\n",
              "      <td>122.33200</td>\n",
              "      <td>-183.75334</td>\n",
              "      <td>89.84912</td>\n",
              "      <td>3.34699</td>\n",
              "      <td>-33.62856</td>\n",
              "      <td>38.22842</td>\n",
              "      <td>77.13977</td>\n",
              "      <td>150.36511</td>\n",
              "      <td>-56.21045</td>\n",
              "      <td>34.03026</td>\n",
              "      <td>-34.86760</td>\n",
              "      <td>60.25658</td>\n",
              "      <td>2.52359</td>\n",
              "      <td>-56.10479</td>\n",
              "      <td>-21.25331</td>\n",
              "      <td>77.13640</td>\n",
              "      <td>134.26121</td>\n",
              "      <td>26.21428</td>\n",
              "      <td>-152.23191</td>\n",
              "      <td>-52.85864</td>\n",
              "      <td>-101.18125</td>\n",
              "      <td>55.52082</td>\n",
              "      <td>-64.97701</td>\n",
              "      <td>227.31886</td>\n",
              "      <td>10.25585</td>\n",
              "      <td>94.90539</td>\n",
              "      <td>15.95689</td>\n",
              "      <td>-98.15732</td>\n",
              "      <td>-9.64859</td>\n",
              "      <td>-93.52834</td>\n",
              "      <td>-95.82981</td>\n",
              "      <td>20.73063</td>\n",
              "      <td>-562.07671</td>\n",
              "      <td>43.44696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1</td>\n",
              "      <td>33.63129</td>\n",
              "      <td>-96.14912</td>\n",
              "      <td>-89.38216</td>\n",
              "      <td>-12.11699</td>\n",
              "      <td>13.77252</td>\n",
              "      <td>-6.69377</td>\n",
              "      <td>-33.36843</td>\n",
              "      <td>-24.81437</td>\n",
              "      <td>21.22757</td>\n",
              "      <td>0.26310</td>\n",
              "      <td>0.42982</td>\n",
              "      <td>-6.59226</td>\n",
              "      <td>27.25596</td>\n",
              "      <td>2407.95516</td>\n",
              "      <td>1257.41135</td>\n",
              "      <td>795.17898</td>\n",
              "      <td>645.83254</td>\n",
              "      <td>1014.07214</td>\n",
              "      <td>474.67866</td>\n",
              "      <td>697.26135</td>\n",
              "      <td>334.85298</td>\n",
              "      <td>358.30744</td>\n",
              "      <td>324.45833</td>\n",
              "      <td>165.58489</td>\n",
              "      <td>-8.08451</td>\n",
              "      <td>101.52431</td>\n",
              "      <td>39.60959</td>\n",
              "      <td>7.06185</td>\n",
              "      <td>-78.21043</td>\n",
              "      <td>88.00470</td>\n",
              "      <td>129.65276</td>\n",
              "      <td>50.90538</td>\n",
              "      <td>-21.26991</td>\n",
              "      <td>32.76996</td>\n",
              "      <td>-29.96947</td>\n",
              "      <td>48.47300</td>\n",
              "      <td>441.40686</td>\n",
              "      <td>454.01182</td>\n",
              "      <td>95.66968</td>\n",
              "      <td>...</td>\n",
              "      <td>55.84917</td>\n",
              "      <td>92.63264</td>\n",
              "      <td>30.35791</td>\n",
              "      <td>-79.20043</td>\n",
              "      <td>-7.46621</td>\n",
              "      <td>602.37381</td>\n",
              "      <td>188.96249</td>\n",
              "      <td>-98.71438</td>\n",
              "      <td>176.37440</td>\n",
              "      <td>-248.00977</td>\n",
              "      <td>44.62138</td>\n",
              "      <td>24.37125</td>\n",
              "      <td>30.40357</td>\n",
              "      <td>79.40089</td>\n",
              "      <td>230.05627</td>\n",
              "      <td>-2.13554</td>\n",
              "      <td>30.01898</td>\n",
              "      <td>-41.89611</td>\n",
              "      <td>-1.77960</td>\n",
              "      <td>46.71421</td>\n",
              "      <td>149.76135</td>\n",
              "      <td>177.92434</td>\n",
              "      <td>-8.49394</td>\n",
              "      <td>47.67341</td>\n",
              "      <td>85.99877</td>\n",
              "      <td>6.63650</td>\n",
              "      <td>-213.70857</td>\n",
              "      <td>90.57614</td>\n",
              "      <td>-15.13552</td>\n",
              "      <td>-99.22940</td>\n",
              "      <td>49.93249</td>\n",
              "      <td>-14.47489</td>\n",
              "      <td>40.70590</td>\n",
              "      <td>58.63692</td>\n",
              "      <td>8.81522</td>\n",
              "      <td>27.28474</td>\n",
              "      <td>5.78046</td>\n",
              "      <td>3.44539</td>\n",
              "      <td>259.10825</td>\n",
              "      <td>10.28525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0</td>\n",
              "      <td>41.38639</td>\n",
              "      <td>-20.78665</td>\n",
              "      <td>51.80155</td>\n",
              "      <td>17.21415</td>\n",
              "      <td>-36.44189</td>\n",
              "      <td>-11.53169</td>\n",
              "      <td>11.75252</td>\n",
              "      <td>-7.62428</td>\n",
              "      <td>-3.65488</td>\n",
              "      <td>-5.08109</td>\n",
              "      <td>4.37624</td>\n",
              "      <td>-1.39493</td>\n",
              "      <td>18.28400</td>\n",
              "      <td>3157.88796</td>\n",
              "      <td>1116.07117</td>\n",
              "      <td>913.19211</td>\n",
              "      <td>638.12946</td>\n",
              "      <td>535.61444</td>\n",
              "      <td>440.68379</td>\n",
              "      <td>435.75048</td>\n",
              "      <td>545.13836</td>\n",
              "      <td>286.93019</td>\n",
              "      <td>154.24744</td>\n",
              "      <td>278.14609</td>\n",
              "      <td>168.02465</td>\n",
              "      <td>-392.26915</td>\n",
              "      <td>-109.59025</td>\n",
              "      <td>-243.33016</td>\n",
              "      <td>57.76966</td>\n",
              "      <td>7.51054</td>\n",
              "      <td>84.23430</td>\n",
              "      <td>-67.00385</td>\n",
              "      <td>80.72070</td>\n",
              "      <td>-0.00321</td>\n",
              "      <td>2.12590</td>\n",
              "      <td>41.51370</td>\n",
              "      <td>-389.17834</td>\n",
              "      <td>-421.90682</td>\n",
              "      <td>-294.54854</td>\n",
              "      <td>...</td>\n",
              "      <td>127.18184</td>\n",
              "      <td>-106.70486</td>\n",
              "      <td>-18.10968</td>\n",
              "      <td>19.19753</td>\n",
              "      <td>-2.79115</td>\n",
              "      <td>31.93573</td>\n",
              "      <td>10.40156</td>\n",
              "      <td>-226.50662</td>\n",
              "      <td>-2.38144</td>\n",
              "      <td>-157.93284</td>\n",
              "      <td>0.46577</td>\n",
              "      <td>27.78523</td>\n",
              "      <td>6.41002</td>\n",
              "      <td>-173.66782</td>\n",
              "      <td>29.73562</td>\n",
              "      <td>-44.29195</td>\n",
              "      <td>-81.53220</td>\n",
              "      <td>50.49506</td>\n",
              "      <td>65.00208</td>\n",
              "      <td>-17.76363</td>\n",
              "      <td>-37.10280</td>\n",
              "      <td>264.10009</td>\n",
              "      <td>100.74257</td>\n",
              "      <td>-57.74383</td>\n",
              "      <td>-22.25492</td>\n",
              "      <td>-17.44555</td>\n",
              "      <td>335.82434</td>\n",
              "      <td>80.63635</td>\n",
              "      <td>-37.20103</td>\n",
              "      <td>7.30588</td>\n",
              "      <td>50.37614</td>\n",
              "      <td>-40.48205</td>\n",
              "      <td>48.07805</td>\n",
              "      <td>-7.62399</td>\n",
              "      <td>6.51934</td>\n",
              "      <td>-30.46090</td>\n",
              "      <td>-53.87264</td>\n",
              "      <td>4.44627</td>\n",
              "      <td>58.16913</td>\n",
              "      <td>-0.02409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0</td>\n",
              "      <td>37.45034</td>\n",
              "      <td>11.42615</td>\n",
              "      <td>56.28982</td>\n",
              "      <td>19.58426</td>\n",
              "      <td>-16.43530</td>\n",
              "      <td>2.22457</td>\n",
              "      <td>1.02668</td>\n",
              "      <td>-7.34736</td>\n",
              "      <td>-0.01184</td>\n",
              "      <td>1.24013</td>\n",
              "      <td>2.57660</td>\n",
              "      <td>2.79598</td>\n",
              "      <td>30.02393</td>\n",
              "      <td>1249.33039</td>\n",
              "      <td>3438.98500</td>\n",
              "      <td>962.85885</td>\n",
              "      <td>1654.99981</td>\n",
              "      <td>1066.80390</td>\n",
              "      <td>606.24732</td>\n",
              "      <td>278.63327</td>\n",
              "      <td>273.36684</td>\n",
              "      <td>250.97765</td>\n",
              "      <td>257.04632</td>\n",
              "      <td>294.90282</td>\n",
              "      <td>41.74146</td>\n",
              "      <td>-455.47604</td>\n",
              "      <td>473.68747</td>\n",
              "      <td>-72.16130</td>\n",
              "      <td>409.83317</td>\n",
              "      <td>111.23583</td>\n",
              "      <td>98.74929</td>\n",
              "      <td>19.15393</td>\n",
              "      <td>-38.69791</td>\n",
              "      <td>-20.10176</td>\n",
              "      <td>57.44465</td>\n",
              "      <td>-127.58760</td>\n",
              "      <td>-189.37100</td>\n",
              "      <td>1126.01208</td>\n",
              "      <td>-98.48301</td>\n",
              "      <td>...</td>\n",
              "      <td>55.86703</td>\n",
              "      <td>-15.58939</td>\n",
              "      <td>-40.62299</td>\n",
              "      <td>-45.18282</td>\n",
              "      <td>-85.17021</td>\n",
              "      <td>180.09138</td>\n",
              "      <td>491.26688</td>\n",
              "      <td>6.42139</td>\n",
              "      <td>263.30124</td>\n",
              "      <td>-152.86190</td>\n",
              "      <td>-16.74493</td>\n",
              "      <td>-33.13223</td>\n",
              "      <td>-10.64964</td>\n",
              "      <td>-367.08218</td>\n",
              "      <td>49.57986</td>\n",
              "      <td>17.60102</td>\n",
              "      <td>-112.39761</td>\n",
              "      <td>79.62965</td>\n",
              "      <td>-92.92839</td>\n",
              "      <td>-49.77861</td>\n",
              "      <td>-53.65950</td>\n",
              "      <td>184.29601</td>\n",
              "      <td>-37.85258</td>\n",
              "      <td>-218.50833</td>\n",
              "      <td>-37.55140</td>\n",
              "      <td>-17.84213</td>\n",
              "      <td>-59.35940</td>\n",
              "      <td>-168.74190</td>\n",
              "      <td>-48.89748</td>\n",
              "      <td>-99.69609</td>\n",
              "      <td>-22.46207</td>\n",
              "      <td>-25.77228</td>\n",
              "      <td>-322.42841</td>\n",
              "      <td>-146.57408</td>\n",
              "      <td>13.61588</td>\n",
              "      <td>92.22918</td>\n",
              "      <td>-439.80259</td>\n",
              "      <td>25.73235</td>\n",
              "      <td>157.22967</td>\n",
              "      <td>38.70617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0</td>\n",
              "      <td>39.71092</td>\n",
              "      <td>-4.92800</td>\n",
              "      <td>12.88590</td>\n",
              "      <td>-11.87773</td>\n",
              "      <td>2.48031</td>\n",
              "      <td>-16.11028</td>\n",
              "      <td>-16.40421</td>\n",
              "      <td>-8.29657</td>\n",
              "      <td>9.86817</td>\n",
              "      <td>-0.17431</td>\n",
              "      <td>0.98765</td>\n",
              "      <td>7.37903</td>\n",
              "      <td>13.84987</td>\n",
              "      <td>1256.34345</td>\n",
              "      <td>800.61127</td>\n",
              "      <td>715.08117</td>\n",
              "      <td>632.19829</td>\n",
              "      <td>357.08486</td>\n",
              "      <td>218.48245</td>\n",
              "      <td>278.90104</td>\n",
              "      <td>229.11644</td>\n",
              "      <td>228.51202</td>\n",
              "      <td>87.35304</td>\n",
              "      <td>144.36923</td>\n",
              "      <td>34.96307</td>\n",
              "      <td>-123.85672</td>\n",
              "      <td>-10.33970</td>\n",
              "      <td>-171.19156</td>\n",
              "      <td>-89.00293</td>\n",
              "      <td>39.72671</td>\n",
              "      <td>-17.78437</td>\n",
              "      <td>4.56124</td>\n",
              "      <td>17.07687</td>\n",
              "      <td>-20.75794</td>\n",
              "      <td>16.49367</td>\n",
              "      <td>63.67926</td>\n",
              "      <td>-113.39747</td>\n",
              "      <td>12.27230</td>\n",
              "      <td>-188.87545</td>\n",
              "      <td>...</td>\n",
              "      <td>-15.69508</td>\n",
              "      <td>1.68706</td>\n",
              "      <td>17.69997</td>\n",
              "      <td>-3.35888</td>\n",
              "      <td>4.21699</td>\n",
              "      <td>25.45729</td>\n",
              "      <td>147.84361</td>\n",
              "      <td>-69.50808</td>\n",
              "      <td>81.47487</td>\n",
              "      <td>-91.89773</td>\n",
              "      <td>9.59723</td>\n",
              "      <td>2.52910</td>\n",
              "      <td>-9.48278</td>\n",
              "      <td>-81.54172</td>\n",
              "      <td>-30.46621</td>\n",
              "      <td>8.10179</td>\n",
              "      <td>9.51480</td>\n",
              "      <td>25.88937</td>\n",
              "      <td>28.29212</td>\n",
              "      <td>5.73829</td>\n",
              "      <td>158.30039</td>\n",
              "      <td>96.20428</td>\n",
              "      <td>55.04618</td>\n",
              "      <td>18.25457</td>\n",
              "      <td>-18.37851</td>\n",
              "      <td>1.06499</td>\n",
              "      <td>-76.40237</td>\n",
              "      <td>111.81246</td>\n",
              "      <td>-57.71450</td>\n",
              "      <td>29.55411</td>\n",
              "      <td>11.92816</td>\n",
              "      <td>-73.72412</td>\n",
              "      <td>16.19039</td>\n",
              "      <td>9.79606</td>\n",
              "      <td>9.71693</td>\n",
              "      <td>-9.90907</td>\n",
              "      <td>-20.65851</td>\n",
              "      <td>2.34002</td>\n",
              "      <td>-31.57015</td>\n",
              "      <td>1.58400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20 rows Ã— 91 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    year      var1       var2  ...     var88       var89     var90\n",
              "0      1  49.94357   21.47114  ...  -1.82223   -27.46348   2.26327\n",
              "1      1  48.73215   18.42930  ...  12.04941    58.43453  26.92061\n",
              "2      1  50.95714   31.85602  ...  -0.05859    39.67068  -0.66345\n",
              "3      1  48.24750   -1.89837  ...   9.90558   199.62971  18.85382\n",
              "4      1  50.97020   42.20998  ...   7.88713    55.66926  28.74903\n",
              "5      1  50.54767    0.31568  ...   5.00283   -11.02257   0.02263\n",
              "6      1  50.57546   33.17843  ...   4.50056    -4.62739   1.40192\n",
              "7      1  48.26892    8.97526  ...  -0.30633     3.98364  -3.72556\n",
              "8      1  49.75468   33.99581  ...   5.48708    -9.13495   6.08680\n",
              "9      1  45.17809   46.34234  ...  11.49326   -89.21804 -15.09719\n",
              "10     1  39.13076  -23.01763  ...  -2.59543   109.19723  23.36143\n",
              "11     1  37.66498  -34.05910  ...  -5.19009     8.83617  -7.16056\n",
              "12     1  26.51957 -148.15762  ...  23.00230  -164.02536  51.54138\n",
              "13     1  37.68491  -26.84185  ...  14.11648 -1030.99180  99.28967\n",
              "14     0  39.11695   -8.29767  ...  -2.14942  -211.48202 -12.81569\n",
              "15     1  35.05129  -67.97714  ...  20.73063  -562.07671  43.44696\n",
              "16     1  33.63129  -96.14912  ...   3.44539   259.10825  10.28525\n",
              "17     0  41.38639  -20.78665  ...   4.44627    58.16913  -0.02409\n",
              "18     0  37.45034   11.42615  ...  25.73235   157.22967  38.70617\n",
              "19     0  39.71092   -4.92800  ...   2.34002   -31.57015   1.58400\n",
              "\n",
              "[20 rows x 91 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncjxI4WdzGrA"
      },
      "source": [
        "### Part (a) -- 7%\n",
        "\n",
        "The data set description text asks us to respect the below train/test split to\n",
        "avoid the \"producer effect\". That is, we want to make sure that no song from a single artist\n",
        "ends up in both the training and test set.\n",
        "\n",
        "Explain why it would be problematic to have\n",
        "some songs from an artist in the training set, and other songs from the same artist in the\n",
        "test set. (Hint: Remember that we want our test accuracy to predict how well the model\n",
        "will perform in practice on a song it hasn't learned about.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NiYlxpFzGrB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "f5cff54b-fb9e-49e0-c930-2fa64e51e793"
      },
      "source": [
        "df_train = df[:463715]\n",
        "df_test = df[463715:]\n",
        "\n",
        "# convert to numpy\n",
        "train_xs = df_train[x_labels].to_numpy()\n",
        "train_ts = df_train[t_label].to_numpy()\n",
        "test_xs = df_test[x_labels].to_numpy()\n",
        "test_ts = df_test[t_label].to_numpy()\n",
        "\n",
        "# Write your explanation here\n",
        "'''\n",
        "To evaluate our model properly, we want to know how well the model will perform over a new songs that it hasn't encountered before.\n",
        "Using songs from the same artist for both the training set and the test set can lead to a situation in which the model learns to extract features based on the singer,\n",
        "e.g., the singer's voice, and will decide based on those features, instead based of the century.\n",
        "In this scenario, the model might perform well on samples of songs in the test where their corresponding singers also appeared in the train set.\n",
        "But this doesn't guarantee that the model will generalize for songs in the test set that their singers didn't appear in the train set.\n",
        "'''\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nTo evaluate our model properly, we want to know how well the model will perform over a new songs that it hasn't encountered before.\\nUsing songs from the same artist for both the training set and the test set can lead to a situation in which the model learns to extract features based on the singer,\\ne.g., the singer's voice, and will decide based on those features, instead based of the century.\\nIn this scenario, the model might perform well on samples of songs in the test where their corresponding singers also appeared in the train set.\\nBut this doesn't guarantee that the model will generalize for songs in the test set that their singers didn't appear in the train set.\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYSzd4XUzGrB"
      },
      "source": [
        "### Part (b) -- 7%\n",
        "\n",
        "It can be beneficial to **normalize** the columns, so that each column (feature)\n",
        "has the *same* mean and standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPuWLksJzGrB"
      },
      "source": [
        "feature_means = df_train.mean()[1:].to_numpy() # the [1:] removes the mean of the \"year\" field\n",
        "feature_stds  = df_train.std()[1:].to_numpy()\n",
        "\n",
        "train_norm_xs = (train_xs - feature_means) / feature_stds\n",
        "test_norm_xs = (test_xs - feature_means) / feature_stds"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4zmZk6ezGrC"
      },
      "source": [
        "Notice how in our code, we normalized the test set using the *training data means and standard deviations*.\n",
        "This is *not* a bug.\n",
        "\n",
        "Explain why it would be improper to compute and use test set means\n",
        "and standard deviations. (Hint: Remember what we want to use the test accuracy to measure.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxZy6brwzGrC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "cbf77a45-54b4-43bf-9903-e1a2057ee5da"
      },
      "source": [
        "# Write your explanation here\n",
        "\"\"\"\n",
        "We normalize the data with respect to the training set to make the input data used for training \"more identical distributed\" during training.\n",
        "We are using test data to evaluate our model performance over new unseen data.\n",
        "If we are using the average and standard deviation of this test set to normalize it, we enforce dependency between different samples in the test set. \n",
        "This harms the idea of model evaluation over new unseen data, e.g., if we evaluate our model for two different test sets, both contain some identical samples.\n",
        "Following this approach will lead to different outcomes for the same input sample (because the average and the standard deviation will be different), which demonstrates the problem with this approach.\n",
        "In addition, in real-time, there is usually a single input and not a set.\n",
        "\"\"\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nWe normalize the data with respect to the training set to make the input data used for training \"more identical distributed\" during training.\\nWe are using test data to evaluate our model performance over new unseen data.\\nIf we are using the average and standard deviation of this test set to normalize it, we enforce dependency between different samples in the test set. \\nThis harms the idea of model evaluation over new unseen data, e.g., if we evaluate our model for two different test sets, both contain some identical samples.\\nFollowing this approach will lead to different outcomes for the same input sample (because the average and the standard deviation will be different), which demonstrates the problem with this approach.\\nIn addition, in real-time, there is usually a single input and not a set.\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4GqL5J_zGrC"
      },
      "source": [
        "### Part (c) -- 7%\n",
        "\n",
        "Finally, we'll move some of the data in our training set into a validation set.\n",
        "\n",
        "Explain why we should limit how many times we use the test set, and that we should use the validation\n",
        "set during the model building process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsXv1U3gzGrC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "8dc86e05-00b8-4007-d323-7375e1f1df6a"
      },
      "source": [
        "# shuffle the training set\n",
        "reindex = np.random.permutation(len(train_xs))\n",
        "train_xs = train_xs[reindex]\n",
        "train_norm_xs = train_norm_xs[reindex]\n",
        "train_ts = train_ts[reindex]\n",
        "\n",
        "# use the first 50000 elements of `train_xs` as the validation set\n",
        "train_xs, val_xs           = train_xs[50000:], train_xs[:50000]\n",
        "train_norm_xs, val_norm_xs = train_norm_xs[50000:], train_norm_xs[:50000]\n",
        "train_ts, val_ts           = train_ts[50000:], train_ts[:50000]\n",
        "\n",
        "# Write your explanation here\n",
        "'''\n",
        "If we will not limit the times that we are using the test set, we may encounter overfitting phenomena over the test data, \n",
        "which in turn will lead to good preformance over the test set, this phenomena may occur becuase we are tuning our model to achieve\n",
        "good preformance over a specific set of test samples. \n",
        "This is a problem because the main reason that we are using a test set is to evaluate our model preformance over an unseen data,\n",
        "i.e., we test how well the training procedure will be able to generalize for unseen samples. \n",
        "We can avoid this issue by examing our model preformance (during the bulding procedure) via\n",
        "another set which we call \"validation\" set. The validation dataset is different from the test dataset, \n",
        "both datasets are held back from the training of the model,but the validation set used to give an unbiased preformance estimation\n",
        "of the final tuned model when comparing between different final models.\"\n",
        "'''"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nIf we will not limit the times that we are using the test set, we may encounter overfitting phenomena over the test data, \\nwhich in turn will lead to good preformance over the test set, this phenomena may occur becuase we are tuning our model to achieve\\ngood preformance over a specific set of test samples. \\nThis is a problem because the main reason that we are using a test set is to evaluate our model preformance over an unseen data,\\ni.e., we test how well the training procedure will be able to generalize for unseen samples. \\nWe can avoid this issue by examing our model preformance (during the bulding procedure) via\\nanother set which we call \"validation\" set. The validation dataset is different from the test dataset, \\nboth datasets are held back from the training of the model,but the validation set used to give an unbiased preformance estimation\\nof the final tuned model when comparing between different final models.\"\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy4lt445zGrD"
      },
      "source": [
        "## Part 2. Classification (79%)\n",
        "\n",
        "We will first build a *classification* model to perform decade classification.\n",
        "These helper functions are written for you. All other code that you write in this section should be vectorized whenever possible (i.e., avoid unnecessary loops)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6BA_s-kzGrD"
      },
      "source": [
        "def sigmoid(z):\n",
        "  return 1 / (1 + np.exp(-z))\n",
        "    \n",
        "def cross_entropy(t, y):\n",
        "  epsilon=1e-15 #to avoid log(0)\n",
        "  return -t * np.log(y+epsilon) - (1 - t) * np.log(1 - y+epsilon)\n",
        "\n",
        "def cost(y, t):\n",
        "  return np.mean(cross_entropy(t, y))\n",
        "\n",
        "def get_accuracy(y, t):\n",
        "  acc = 0\n",
        "  N = 0\n",
        "  for i in range(len(y)):\n",
        "    N += 1\n",
        "    if (y[i] >= 0.5 and t[i] == 1) or (y[i] < 0.5 and t[i] == 0):\n",
        "      acc += 1\n",
        "  return acc / N"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8ZIfooBzGrD"
      },
      "source": [
        "### Part (a) -- 7%\n",
        "\n",
        "Write a function `pred` that computes the prediction `y` based on logistic regression, i.e., a single layer with weights `w` and bias `b`. The output is given by: \n",
        "\\begin{equation}\n",
        "y = \\sigma({\\bf w}^T {\\bf x} + b),\n",
        "\\end{equation}\n",
        "where the value of $y$ is an estimate of the probability that the song is released in the current century, namely ${\\rm year} =1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naY5mT4_zGrD"
      },
      "source": [
        "def pred(w, b, X):\n",
        "  \"\"\"\n",
        "  Returns the prediction `y` of the target based on the weights `w` and scalar \n",
        "  bias `b`.\n",
        "\n",
        "  Preconditions: np.shape(w) == (90,)\n",
        "                 type(b) == float\n",
        "                 np.shape(X) = (N, 90) for some N\n",
        "\n",
        "  >>> pred(np.zeros(90), 1, np.ones([2, 90]))\n",
        "  array([0.73105858, 0.73105858]) # It's okay if your output differs in the last\n",
        "  decimals\n",
        "  \"\"\"\n",
        "  return sigmoid(np.dot(X,w)+b)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxNdmSd3zGrE"
      },
      "source": [
        "### Part (b) -- 7%\n",
        "\n",
        "Write a function `derivative_cost` that computes and returns the gradients \n",
        "$\\frac{\\partial\\mathcal{L}}{\\partial {\\bf w}}$ and\n",
        "$\\frac{\\partial\\mathcal{L}}{\\partial b}$. Here, `X` is the input, `y` is the prediction, and `t` is the true label.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P80bu7qmzGrE"
      },
      "source": [
        "def derivative_cost(X, y, t):\n",
        "  \"\"\"\n",
        "  Returns a tuple containing the gradients dLdw and dLdb.\n",
        "\n",
        "  Precondition: np.shape(X) == (N, 90) for some N\n",
        "                np.shape(y) == (N,)\n",
        "                np.shape(t) == (N,)\n",
        "\n",
        "  Postcondition: np.shape(dLdw) = (90,)\n",
        "           type(dLdb) = float\n",
        "  \"\"\"\n",
        "  N = np.shape(y)[0]\n",
        "  dLdb = np.mean(y - t)\n",
        "  dLdw = 1/N * np.dot(y - t,X)\n",
        "  return (dLdw, dLdb)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okPRGM3BjKe2"
      },
      "source": [
        "# **Explenation on Gradients**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHfmPVdsg0eX"
      },
      "source": [
        "First we define our loss function as followed:\n",
        "\n",
        "$L(w,b)=-\\frac{1}{N}\\sum_{n=1}^{N}t_n\\log(y_n)+(1-t_n)\\log(1-y_n)$.\n",
        "\n",
        "Let $z_n = \\mathbf{w}^T\\mathbf{x}_n+b$\n",
        "\n",
        "where we denote bold letter as a vector.\n",
        "\n",
        "By the chain-rule:\n",
        "\n",
        "$\\frac{\\partial L}{\\partial w}= \\sum_{n=1}^{N}\\frac{\\partial L}{\\partial y_n}\\cdot \\frac{\\partial y_n}{\\partial z_n} \\cdot \\frac{\\partial z_n}{\\partial w}$\n",
        "\n",
        "$\\frac{\\partial L}{\\partial b}= \\sum_{n=1}^{N} \\frac{\\partial L}{\\partial y_n}\\cdot \\frac{\\partial y_n}{\\partial z_n} \\cdot \\frac{\\partial z_n}{\\partial b}$ \n",
        "\n",
        "where:\n",
        "\n",
        "$\\frac{\\partial L}{\\partial y_n}=-\\frac{1}{N}(\\frac{t_n}{y_n}-\\frac{1-t_n}{1-y_n} )=\\frac{y_n-t_n}{N \\cdot y_n \\cdot (1-y_n)}$,\n",
        "\n",
        "$\\frac{\\partial y_n}{\\partial z_n} = \\sigma'(z_n) = \\sigma(z_n)(1-\\sigma(z_n))=y_n(1-y_n)$\n",
        "\n",
        "$\\frac{\\partial z_n}{\\partial b} =1$,\n",
        "\n",
        "$\\frac{\\partial z_n}{\\partial w}=\\mathbf{x}_n$\n",
        "\n",
        "These boils down to:\n",
        "\n",
        "$\\frac{\\partial L}{\\partial b} = \\sum_{n=1}^{N} \\frac{y_n-t_n}{N \\cdot y_n \\cdot (1-y_n)} \\cdot y_n(1-y_n) \\cdot 1 = \\frac{1}{N}\\sum_{n=1}^{N}y_n-t_n$\n",
        "\n",
        "$\\frac{\\partial L}{\\partial w} = \\sum_{n=1}^{N} \\frac{y_n-t_n}{N \\cdot y_n \\cdot (1-y_n)} \\cdot y_n(1-y_n) \\cdot \\mathbf{x}_b = \\frac{1}{N}\\sum_{n=1}^{N}(y_n-t_n)\\mathbf{x}_n$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhQXAKd4zGrE"
      },
      "source": [
        "### Part (c) -- 7%\n",
        "\n",
        "We can check that our derivative is implemented correctly using the finite difference rule. In 1D, the\n",
        "finite difference rule tells us that for small $h$, we should have\n",
        "\n",
        "$$\\frac{f(x+h) - f(x)}{h} \\approx f'(x)$$\n",
        "\n",
        "Show that $\\frac{\\partial\\mathcal{L}}{\\partial b}$  is implement correctly\n",
        "by comparing the result from `derivative_cost` with the empirical cost derivative computed using the above numerical approximation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpRTD-fozGrF"
      },
      "source": [
        "# Your code goes here\n",
        "def compute_analytical_deriviate(w,b,t,X,h=1e-05):\n",
        "  '''\n",
        "  First we calculate the derivative with respect to the bias term\n",
        "  '''\n",
        "  # Calculate the predications\n",
        "  y = pred(w, b, X) \n",
        "  # Calculate the loss of those predications\n",
        "  L = cost(y,t)\n",
        "  y_new = pred(w,b+h,X)\n",
        "  L_new = cost(y_new,t)\n",
        "\n",
        "  # Calculate the analytic derivative\n",
        "  deriviate_b =(L_new-L)/h\n",
        "\n",
        "  '''\n",
        "  Now we calculate the derivative with respect to the weights\n",
        "  '''\n",
        "  L_new = np.zeros(len(w))\n",
        "  deriviate_w = np.zeros(len(w))\n",
        "  for weight_idx in range(len(w)):\n",
        "    # Compute cost of w[weight_idx] + h\n",
        "    w_new = np.copy(w)\n",
        "    w_new[weight_idx] = w_new[weight_idx] + h\n",
        "    # Calculate the predications\n",
        "    y_new = pred(w_new,b,X)\n",
        "    # Calculate the loss of those predications\n",
        "    L_new[weight_idx]=cost(y_new,t)\n",
        "    deriviate_w[weight_idx] = (L_new[weight_idx]-L)/h\n",
        "  \n",
        "  return deriviate_w, deriviate_b\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU__UXAnkWU9"
      },
      "source": [
        "Now we are going to check our derivative using an example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiL3lpuzlO03",
        "outputId": "5a88b83e-7388-4655-9014-d451ee5ae742"
      },
      "source": [
        "w=np.zeros(90)\n",
        "h=1e-05\n",
        "b=1\n",
        "t=np.array([1,1])\n",
        "X=np.ones([2, 90])\n",
        "y = pred(w, b, X)\n",
        "dw,db = derivative_cost(X,y,t)\n",
        "dw_analytic, db_analytic = compute_analytical_deriviate(w,b,t,X,h)\n",
        "print(\"The bias analytical derivative results is -\", db_analytic)\n",
        "print(\"The bias algorithm derivative results is - \", db)\n",
        "print(f\"The error is {np.abs(db-db_analytic)}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The bias analytical derivative results is - -0.26894043830827385\n",
            "The bias algorithm derivative results is -  -0.2689414213699951\n",
            "The error is 9.830617212491788e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTiplTPhzGrF"
      },
      "source": [
        "### Part (d) -- 7%\n",
        "\n",
        "Show that $\\frac{\\partial\\mathcal{L}}{\\partial {\\bf w}}$  is implement correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVTsHgnPzGrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40a9e293-dcad-4d84-fd34-e9616ccea3b1"
      },
      "source": [
        "print(\"The weights analytical derivative results is -\", dw_analytic)\n",
        "print(\"The weights algorithm derivative results is - \", dw)\n",
        "print(f\"The maximal error is {np.max(np.abs(dw-dw_analytic))}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The weights analytical derivative results is - [-0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044\n",
            " -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044\n",
            " -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044\n",
            " -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044\n",
            " -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044\n",
            " -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044\n",
            " -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044\n",
            " -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044\n",
            " -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044\n",
            " -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044\n",
            " -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044\n",
            " -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044\n",
            " -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044\n",
            " -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044\n",
            " -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044]\n",
            "The weights algorithm derivative results is -  [-0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n",
            " -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142]\n",
            "The maximal error is 9.830617212491788e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saUYqefgmc4J"
      },
      "source": [
        "Now we will check over couple of real examples from the training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7Ry_wcZmJnd",
        "outputId": "b269e2c0-0569-4e7b-bf50-dd0478993aaf"
      },
      "source": [
        "h = 1e-9;\n",
        "# First we initialize the weights by random numbers\n",
        "w=np.random.rand(90)\n",
        "b=np.random.rand(1)\n",
        "\n",
        "# Now we extract the actual samples from the dataset\n",
        "t = (train_ts[:1]).squeeze()\n",
        "X = train_norm_xs[:1]\n",
        "y = pred(w, b, X)\n",
        "\n",
        "dw,db = derivative_cost(X,y,t)\n",
        "dw_analytic, db_analytic = compute_analytical_deriviate(w,b,t,X,h)\n",
        "print(\"The analytical derivative bias results is -\", db_analytic)\n",
        "print(\"The algorithm derivative bias results is - \", db)\n",
        "print(f\"Bias Error is {np.abs(db-db_analytic)}\")\n",
        "\n",
        "print(\"The analytical derivative weights results is -\", dw_analytic)\n",
        "print(\"The algorithm derivative weights results is - \", dw)\n",
        "print(f\"Maximal weights Error is {np.max(np.abs(dw-dw_analytic))}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The analytical derivative bias results is - -0.9672760370449395\n",
            "The algorithm derivative bias results is -  -0.9672759967800327\n",
            "Bias Error is 4.0264906742137896e-08\n",
            "The analytical derivative weights results is - [ 0.97880459  1.33712197  1.5150321   1.17287824 -0.1407332  -0.28086733\n",
            "  0.99645137  1.66069425  0.34143222  2.53612864 -0.60000049  0.2660081\n",
            " -0.16358959  0.27339908 -0.18315438 -0.02620393  0.57133454  0.29290792\n",
            " -0.22191848 -0.26613023  0.32624214 -0.12356471  0.2247269  -0.1315108\n",
            "  0.79538109  0.54556981  0.37574432  0.19421131  0.50898086  0.31249137\n",
            " -0.95565378 -0.54595706  0.15626922  0.11944934 -0.86183682 -0.96469277\n",
            " -0.30012703 -0.25347191 -0.2640399  -0.20129987  0.12283019  0.02585665\n",
            " -0.75847728  0.77280982  0.88499963 -0.2634799   0.25427704  0.86388496\n",
            " -0.74310602 -0.05215162  0.02074074 -0.18614754  0.64476735  0.68632078\n",
            " -0.79160367 -0.5105103  -0.51600768  0.62609251 -0.36517989  1.05336495\n",
            "  0.91098018  0.16509771  0.39282799  0.36208059 -1.05262732  0.20108226\n",
            "  0.01534106 -0.28936853  0.35263614 -0.85813934  0.51580074 -0.01067191\n",
            " -0.41914161  0.52804738 -0.2147611  -0.83647089  0.07340262 -1.70637771\n",
            " -0.25002356  0.29500091  0.20058133 -0.08705836  0.93793728  0.12571144\n",
            " -1.7977615  -0.31657921  0.64228134  0.73033668 -0.51113247  0.58821525]\n",
            "The algorithm derivative weights results is -  [ 0.97880513  1.33712198  1.51503252  1.17287858 -0.14073285 -0.28086701\n",
            "  0.99645136  1.66069465  0.34143218  2.53612882 -0.60000029  0.26600803\n",
            " -0.16358953  0.27339904 -0.18315388 -0.02620335  0.57133477  0.29290862\n",
            " -0.22191785 -0.26613003  0.3262425  -0.12356409  0.22472752 -0.13151021\n",
            "  0.79538094  0.54556964  0.37574461  0.19421122  0.5089815   0.31249165\n",
            " -0.95565349 -0.54595737  0.15626943  0.11944995 -0.86183685 -0.96469248\n",
            " -0.30012698 -0.25347199 -0.26404002 -0.20129932  0.12282997  0.02585716\n",
            " -0.75847651  0.77280993  0.88499994 -0.26347972  0.25427681  0.86388554\n",
            " -0.74310507 -0.05215134  0.02074135 -0.18614762  0.64476718  0.68632109\n",
            " -0.79160365 -0.5105102  -0.51600786  0.62609255 -0.36517956  1.05336573\n",
            "  0.91098042  0.16509771  0.39282848  0.3620809  -1.05262721  0.20108315\n",
            "  0.01534155 -0.28936764  0.35263587 -0.8581389   0.51580074 -0.01067161\n",
            " -0.41914165  0.52804727 -0.21476141 -0.83647112  0.07340269 -1.70637775\n",
            " -0.25002302  0.29500102  0.20058141 -0.08705784  0.93793721  0.12571158\n",
            " -1.79776144 -0.31657967  0.64228073  0.73033717 -0.51113169  0.58821569]\n",
            "Maximal weights Error is 9.504835127849276e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgBTPF_2zGrG"
      },
      "source": [
        "### Part (e) -- 7%\n",
        "\n",
        "Now that you have a gradient function that works, we can actually run gradient descent. \n",
        "Complete the following code that will run stochastic: gradient descent training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW4DEuuPzGrG"
      },
      "source": [
        "def run_gradient_descent(w0, b0, mu=0.1, batch_size=100, max_iters=100):\n",
        "  \"\"\"Return the values of (w, b) after running gradient descent for max_iters.\n",
        "  We use:\n",
        "    - train_norm_xs and train_ts as the training set\n",
        "    - val_norm_xs and val_ts as the test set\n",
        "    - mu as the learning rate\n",
        "    - (w0, b0) as the initial values of (w, b)\n",
        "\n",
        "  Precondition: np.shape(w0) == (90,)\n",
        "                type(b0) == float\n",
        " \n",
        "  Postcondition: np.shape(w) == (90,)\n",
        "                 type(b) == float\n",
        "  \"\"\"\n",
        "  w = w0\n",
        "  b = b0\n",
        "  iter = 0\n",
        "  val_loss = []\n",
        "  \n",
        "  while iter < max_iters:\n",
        "    # shuffle the training set (there is code above for how to do this)\n",
        "    new_indices = np.random.permutation(len(train_norm_xs))\n",
        "    train_norm_xs_temp = train_norm_xs[new_indices]\n",
        "    train_ts_temp = train_ts[new_indices]\n",
        "\n",
        "    for i in range(0, len(train_norm_xs), batch_size): # iterate over each minibatch\n",
        "      # minibatch that we are working with:\n",
        "      X = train_norm_xs_temp[i:(i + batch_size)]\n",
        "      t = train_ts_temp[i:(i + batch_size), 0]\n",
        "\n",
        "      # since len(train_norm_xs) does not divide batch_size evenly, we will skip over\n",
        "      # the \"last\" minibatch\n",
        "      if np.shape(X)[0] != batch_size:\n",
        "        continue\n",
        "\n",
        "      # compute the prediction\n",
        "      y = pred(w, b, X)\n",
        "      # update w and b\n",
        "      dw,db = derivative_cost(X,y,t)\n",
        "      w -= mu*dw\n",
        "      b -= mu*db\n",
        "\n",
        "      # increment the iteration count\n",
        "      iter += 1\n",
        "      # compute and print the *validation* loss and accuracy\n",
        "      if (iter % 10 == 0):\n",
        "        # Calculate the predications over the validation data\n",
        "        y_val = pred(w,b,val_norm_xs)\n",
        "        # Calculate the cost over the predications \n",
        "        val_cost = cost(y_val,val_ts.squeeze())\n",
        "        val_acc = get_accuracy(y_val, val_ts)\n",
        "        print(\"Iter %d. [Val Acc %.0f%%, Loss %f]\" % (iter, val_acc * 100, val_cost))\n",
        "        val_loss.append(val_cost)\n",
        "      if iter >= max_iters:\n",
        "        break\n",
        "\n",
        "      # Think what parameters you should return for further use\n",
        "      # We choose to return in addition to w and b the validation loss in order to evaluate our model training procedure over unseen data.\n",
        "  return w, b, val_loss\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MqzT0jGzGrH"
      },
      "source": [
        "### Part (f) -- 7%\n",
        "\n",
        "Call `run_gradient_descent` with the weights and biases all initialized to zero.\n",
        "Show that if the learning rate $\\mu$ is too small, then convergence is slow.\n",
        "Also, show that if $\\mu$ is too large, then the optimization algorirthm does not converge. The demonstration should be made using plots showing these effects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tE32Iqo6zGrH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 924
        },
        "outputId": "c5581b13-108b-4989-a2b8-471a622fd3f2"
      },
      "source": [
        "w0 = np.zeros(90)\n",
        "b0 = 0\n",
        "Iterations = 100\n",
        "mus=[0.005,0.1,5]\n",
        "for mu in mus:\n",
        "  print(\"mu=\",mu,\":\")\n",
        "  w,b,losses=run_gradient_descent(w0,b0,mu, max_iters=Iterations)\n",
        "  print(\"-------------------------\")\n",
        "  plt.semilogy(range(0,Iterations,10),losses)\n",
        "\n",
        "plt.legend(mus,loc='best')\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid()\n",
        "plt.title(\"Learning rate convergance properties\")\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mu= 0.005 :\n",
            "Iter 10. [Val Acc 64%, Loss 0.689628]\n",
            "Iter 20. [Val Acc 65%, Loss 0.686469]\n",
            "Iter 30. [Val Acc 64%, Loss 0.683016]\n",
            "Iter 40. [Val Acc 65%, Loss 0.679891]\n",
            "Iter 50. [Val Acc 65%, Loss 0.677516]\n",
            "Iter 60. [Val Acc 65%, Loss 0.675009]\n",
            "Iter 70. [Val Acc 65%, Loss 0.672270]\n",
            "Iter 80. [Val Acc 65%, Loss 0.670617]\n",
            "Iter 90. [Val Acc 66%, Loss 0.668337]\n",
            "Iter 100. [Val Acc 67%, Loss 0.666638]\n",
            "-------------------------\n",
            "mu= 0.1 :\n",
            "Iter 10. [Val Acc 67%, Loss 0.641812]\n",
            "Iter 20. [Val Acc 69%, Loss 0.628086]\n",
            "Iter 30. [Val Acc 69%, Loss 0.617252]\n",
            "Iter 40. [Val Acc 70%, Loss 0.609721]\n",
            "Iter 50. [Val Acc 70%, Loss 0.603973]\n",
            "Iter 60. [Val Acc 71%, Loss 0.598000]\n",
            "Iter 70. [Val Acc 70%, Loss 0.596090]\n",
            "Iter 80. [Val Acc 71%, Loss 0.589885]\n",
            "Iter 90. [Val Acc 72%, Loss 0.586939]\n",
            "Iter 100. [Val Acc 72%, Loss 0.588597]\n",
            "-------------------------\n",
            "mu= 5 :\n",
            "Iter 10. [Val Acc 64%, Loss 1.473788]\n",
            "Iter 20. [Val Acc 65%, Loss 1.890297]\n",
            "Iter 30. [Val Acc 63%, Loss 1.754046]\n",
            "Iter 40. [Val Acc 63%, Loss 1.565325]\n",
            "Iter 50. [Val Acc 67%, Loss 1.543377]\n",
            "Iter 60. [Val Acc 57%, Loss 2.886677]\n",
            "Iter 70. [Val Acc 64%, Loss 1.894496]\n",
            "Iter 80. [Val Acc 56%, Loss 3.832907]\n",
            "Iter 90. [Val Acc 62%, Loss 2.399400]\n",
            "Iter 100. [Val Acc 66%, Loss 1.503188]\n",
            "-------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Learning rate convergance properties')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S44GmUZ12o-g"
      },
      "source": [
        "We conclude from the figure above that for a small learning rate, the algorithm may be capable to converge but very slowly due to the small changes in the parameters space. On the other hand, we see that by using too large learning rate, the algorithm is incapable to converge due to the large changes in the parameters space. Thus, we conclude from the empirical results that choosing a learning rate somewhere in between those learning rates leads to the best performance. \n",
        "We explain this phenomenon by the fact that the algorithm can change its parameters efficiently towards a good optimum. Meaning that we have found a good trade-off between a small enough step size that allows convergence but not too small such that it leads to faster convergence rate to a good enough optimum.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ5HlDFAzGrH"
      },
      "source": [
        "### Part (g) -- 7%\n",
        "\n",
        "Find the optimial value of ${\\bf w}$ and $b$ using your code. Explain how you chose\n",
        "the learning rate $\\mu$ and the batch size. Show plots demostrating good and bad behaviours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dFOFSwgzGrI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2357e75e-c5d3-489f-818e-5b623c92a2ca"
      },
      "source": [
        "'''\n",
        "Hyperparameters search\n",
        "'''\n",
        "\n",
        "mus = [0.1, 0.5, 1]\n",
        "batch_sizes = [64, 512, 1024]\n",
        "Iterations = 100\n",
        "loss_results = np.zeros((len(mus),len(batch_sizes)))\n",
        "w_history = np.zeros((len(mus),len(batch_sizes), 90))\n",
        "b_history = np.zeros((len(mus),len(batch_sizes), 1))\n",
        "\n",
        "w0 = np.random.randn(90)\n",
        "b0 = np.random.randn(1)[0]\n",
        "\n",
        "for mu_idx,mu in enumerate(mus):\n",
        "  for minibatch_idx, minibatch_size in enumerate(batch_sizes):\n",
        "    print(\"mu=\",mu, 'batch_size=', minibatch_size, ':')\n",
        "    w,b,losses=run_gradient_descent(w0,b0,mu, max_iters=Iterations)\n",
        "    print(\"-------------------------\")\n",
        "    plt.plot(list(range(0,Iterations,10)), losses, label=r'$\\mu=$'+str(mu)+r', $BS=$'+str(minibatch_size))\n",
        "    loss_results[mu_idx][minibatch_idx] = losses[-1]\n",
        "    w_history[mu_idx][minibatch_idx] = w\n",
        "    b_history[mu_idx][minibatch_idx] = b\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Label')\n",
        "plt.title('Loss VS Iteration for various learning rates and batch sizes')\n",
        "plt.ylim([0.5,2.5])\n",
        "best_mu_idx, best_batch_size_idx = np.unravel_index(np.argmin(loss_results, axis=None), loss_results.shape)\n",
        "print(\"optimal mu=\",mus[best_mu_idx], 'optimal batch_size=', batch_sizes[best_batch_size_idx])\n",
        "'''\n",
        "Save the optimal weight and bias\n",
        "'''\n",
        "opt_w = w_history[best_mu_idx][best_batch_size_idx]\n",
        "opt_b = b_history[best_mu_idx][best_batch_size_idx]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mu= 0.1 batch_size= 64 :\n",
            "Iter 10. [Val Acc 49%, Loss 3.336478]\n",
            "Iter 20. [Val Acc 50%, Loss 3.031846]\n",
            "Iter 30. [Val Acc 51%, Loss 2.767435]\n",
            "Iter 40. [Val Acc 51%, Loss 2.534040]\n",
            "Iter 50. [Val Acc 52%, Loss 2.334944]\n",
            "Iter 60. [Val Acc 53%, Loss 2.140430]\n",
            "Iter 70. [Val Acc 54%, Loss 1.970409]\n",
            "Iter 80. [Val Acc 54%, Loss 1.837026]\n",
            "Iter 90. [Val Acc 55%, Loss 1.714708]\n",
            "Iter 100. [Val Acc 56%, Loss 1.597602]\n",
            "-------------------------\n",
            "mu= 0.1 batch_size= 512 :\n",
            "Iter 10. [Val Acc 56%, Loss 1.550912]\n",
            "Iter 20. [Val Acc 57%, Loss 1.463362]\n",
            "Iter 30. [Val Acc 57%, Loss 1.378931]\n",
            "Iter 40. [Val Acc 58%, Loss 1.303234]\n",
            "Iter 50. [Val Acc 59%, Loss 1.234784]\n",
            "Iter 60. [Val Acc 60%, Loss 1.167155]\n",
            "Iter 70. [Val Acc 60%, Loss 1.101859]\n",
            "Iter 80. [Val Acc 61%, Loss 1.048859]\n",
            "Iter 90. [Val Acc 62%, Loss 0.992238]\n",
            "Iter 100. [Val Acc 62%, Loss 0.946392]\n",
            "-------------------------\n",
            "mu= 0.1 batch_size= 1024 :\n",
            "Iter 10. [Val Acc 61%, Loss 0.968837]\n",
            "Iter 20. [Val Acc 62%, Loss 0.921298]\n",
            "Iter 30. [Val Acc 63%, Loss 0.886855]\n",
            "Iter 40. [Val Acc 64%, Loss 0.865773]\n",
            "Iter 50. [Val Acc 64%, Loss 0.829914]\n",
            "Iter 60. [Val Acc 65%, Loss 0.804494]\n",
            "Iter 70. [Val Acc 65%, Loss 0.786047]\n",
            "Iter 80. [Val Acc 66%, Loss 0.769052]\n",
            "Iter 90. [Val Acc 67%, Loss 0.751647]\n",
            "Iter 100. [Val Acc 67%, Loss 0.737800]\n",
            "-------------------------\n",
            "mu= 0.5 batch_size= 64 :\n",
            "Iter 10. [Val Acc 67%, Loss 0.733840]\n",
            "Iter 20. [Val Acc 68%, Loss 0.682606]\n",
            "Iter 30. [Val Acc 69%, Loss 0.652998]\n",
            "Iter 40. [Val Acc 70%, Loss 0.644629]\n",
            "Iter 50. [Val Acc 71%, Loss 0.626147]\n",
            "Iter 60. [Val Acc 71%, Loss 0.620681]\n",
            "Iter 70. [Val Acc 71%, Loss 0.622943]\n",
            "Iter 80. [Val Acc 71%, Loss 0.608826]\n",
            "Iter 90. [Val Acc 72%, Loss 0.600933]\n",
            "Iter 100. [Val Acc 71%, Loss 0.601527]\n",
            "-------------------------\n",
            "mu= 0.5 batch_size= 512 :\n",
            "Iter 10. [Val Acc 71%, Loss 0.607369]\n",
            "Iter 20. [Val Acc 72%, Loss 0.602029]\n",
            "Iter 30. [Val Acc 71%, Loss 0.610226]\n",
            "Iter 40. [Val Acc 72%, Loss 0.590211]\n",
            "Iter 50. [Val Acc 72%, Loss 0.582704]\n",
            "Iter 60. [Val Acc 71%, Loss 0.605284]\n",
            "Iter 70. [Val Acc 72%, Loss 0.586659]\n",
            "Iter 80. [Val Acc 72%, Loss 0.577313]\n",
            "Iter 90. [Val Acc 72%, Loss 0.589994]\n",
            "Iter 100. [Val Acc 71%, Loss 0.602749]\n",
            "-------------------------\n",
            "mu= 0.5 batch_size= 1024 :\n",
            "Iter 10. [Val Acc 71%, Loss 0.604912]\n",
            "Iter 20. [Val Acc 71%, Loss 0.597832]\n",
            "Iter 30. [Val Acc 72%, Loss 0.589865]\n",
            "Iter 40. [Val Acc 72%, Loss 0.583877]\n",
            "Iter 50. [Val Acc 72%, Loss 0.584915]\n",
            "Iter 60. [Val Acc 72%, Loss 0.587464]\n",
            "Iter 70. [Val Acc 72%, Loss 0.581562]\n",
            "Iter 80. [Val Acc 71%, Loss 0.592735]\n",
            "Iter 90. [Val Acc 72%, Loss 0.577472]\n",
            "Iter 100. [Val Acc 72%, Loss 0.588344]\n",
            "-------------------------\n",
            "mu= 1 batch_size= 64 :\n",
            "Iter 10. [Val Acc 65%, Loss 0.701522]\n",
            "Iter 20. [Val Acc 71%, Loss 0.606007]\n",
            "Iter 30. [Val Acc 71%, Loss 0.621109]\n",
            "Iter 40. [Val Acc 70%, Loss 0.623334]\n",
            "Iter 50. [Val Acc 69%, Loss 0.625957]\n",
            "Iter 60. [Val Acc 70%, Loss 0.634948]\n",
            "Iter 70. [Val Acc 65%, Loss 0.723293]\n",
            "Iter 80. [Val Acc 70%, Loss 0.624681]\n",
            "Iter 90. [Val Acc 67%, Loss 0.734741]\n",
            "Iter 100. [Val Acc 70%, Loss 0.637793]\n",
            "-------------------------\n",
            "mu= 1 batch_size= 512 :\n",
            "Iter 10. [Val Acc 68%, Loss 0.669947]\n",
            "Iter 20. [Val Acc 69%, Loss 0.678236]\n",
            "Iter 30. [Val Acc 70%, Loss 0.630204]\n",
            "Iter 40. [Val Acc 71%, Loss 0.631502]\n",
            "Iter 50. [Val Acc 68%, Loss 0.675860]\n",
            "Iter 60. [Val Acc 71%, Loss 0.651690]\n",
            "Iter 70. [Val Acc 71%, Loss 0.637303]\n",
            "Iter 80. [Val Acc 71%, Loss 0.608844]\n",
            "Iter 90. [Val Acc 70%, Loss 0.618629]\n",
            "Iter 100. [Val Acc 71%, Loss 0.605964]\n",
            "-------------------------\n",
            "mu= 1 batch_size= 1024 :\n",
            "Iter 10. [Val Acc 71%, Loss 0.642338]\n",
            "Iter 20. [Val Acc 71%, Loss 0.611587]\n",
            "Iter 30. [Val Acc 72%, Loss 0.611512]\n",
            "Iter 40. [Val Acc 71%, Loss 0.604707]\n",
            "Iter 50. [Val Acc 70%, Loss 0.660838]\n",
            "Iter 60. [Val Acc 72%, Loss 0.608936]\n",
            "Iter 70. [Val Acc 70%, Loss 0.618162]\n",
            "Iter 80. [Val Acc 71%, Loss 0.624844]\n",
            "Iter 90. [Val Acc 70%, Loss 0.619128]\n",
            "Iter 100. [Val Acc 71%, Loss 0.614078]\n",
            "-------------------------\n",
            "optimal mu= 0.5 optimal batch_size= 1024\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8ij82PbM_gf"
      },
      "source": [
        "We found the optimal weights and biases by scanning over a range of different learning rates and batch sizes for 100 iterations each and then picking the weights and biases in which the loss over the validation set was minimal.\n",
        "The best weights and biases were observed when $\\mu=0.5$ and batch_size = 1024. The best results are achieved when the batch size is maximal. The reason is that the number of iterations is the number of update steps. Because this number is constant (100), we have the same number of updates independently of the batch size. Thus, when the batch size is large, as we learned in the lectures, we get a better approximation of the true gradient, and hence the results are better.\n",
        "It is important to mention that we should not conclude that the best choice is always picking the largest batch size. The reason is that if we measure the validation loss per epoch (for a fixed number of epochs), the number of update steps with a large batch size will be smaller than the number of updates when the batch size is smaller which may lead to worse performance. Furthermore, a smaller batch size can assist the algorithm in avoiding it from stuck in a local minimum or a saddle point due to the noisier gradient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KrQqSj2zGrI"
      },
      "source": [
        "### Part (h) -- 15%\n",
        "\n",
        "Using the values of `w` and `b` from part (g), compute your training accuracy, validation accuracy,\n",
        "and test accuracy. Are there any differences between those three values? If so, why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuKw2mLozGrI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eadaeb13-6fe2-4c78-e840-610919530467"
      },
      "source": [
        "train_ys=pred(opt_w,opt_b,train_norm_xs)\n",
        "test_ys=pred(opt_w,opt_b,test_norm_xs)\n",
        "val_ys=pred(opt_w,opt_b,val_norm_xs)\n",
        "\n",
        "\n",
        "train_acc = get_accuracy(train_ys,train_ts)\n",
        "val_acc = get_accuracy(val_ys,val_ts)\n",
        "test_acc =  get_accuracy(test_ys,test_ts)\n",
        "\n",
        "print('train_acc = ', train_acc, ' val_acc = ', val_acc, ' test_acc = ', test_acc)\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.7153982814256191  val_acc =  0.71722  test_acc =  0.7074375363160953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXZa1u6920M3"
      },
      "source": [
        "Both train and validation accuracies are better than the test accuracy, we explain this phenomenoa by the fact that we didn't see the test data during the model building process, so we expect to encounter minor degradation in the model performance over the new unseen data.\n",
        "\n",
        "Moreover, we can see that the gap between the train and test data is not significant and therefore we conclude that the model can extract meaningful information from the training dataset.\n",
        "Meaning that the model is capable to perform similarly over unseen data, e.g., the model is well generalized. \n",
        "\n",
        "Regarding the validation accuracy, we have tuned our model hyperparameters(Learning rate and Batch size) such that it achieves the best performance over the validation data, therefore, we shall expect to achieve better results in the validation data than the test data, and this is indeed the result that we've achieved. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4eP2Yh1zGrI"
      },
      "source": [
        "### Part (i) -- 15%\n",
        "\n",
        "Writing a classifier like this is instructive, and helps you understand what happens when\n",
        "we train a model. However, in practice, we rarely write model building and training code\n",
        "from scratch. Instead, we typically use one of the well-tested libraries available in a package.\n",
        "\n",
        "Use `sklearn.linear_model.LogisticRegression` to build a linear classifier, and make predictions about the test set. Start by reading the\n",
        "[API documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
        "\n",
        "Compute the training, validation and test accuracy of this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24LCfAa1zGrJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f2e73b3-91f3-4243-c6fe-a0e3b0c8430b"
      },
      "source": [
        "import sklearn.linear_model\n",
        "\n",
        "model = sklearn.linear_model.LogisticRegression()\n",
        "model.fit(train_norm_xs, train_ts.squeeze())\n",
        "train_ys = model.predict(train_norm_xs)\n",
        "test_ys = model.predict(test_norm_xs)\n",
        "val_ys = model.predict(val_norm_xs)\n",
        "train_acc = get_accuracy(train_ys,train_ts)\n",
        "val_acc = get_accuracy(val_ys,val_ts)\n",
        "test_acc =  get_accuracy(test_ys,test_ts)\n",
        "\n",
        "print('train_acc = ', train_acc, ' val_acc = ', val_acc, ' test_acc = ', test_acc)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.7323979067715698  val_acc =  0.73642  test_acc =  0.7265736974627155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRqucdV923tG"
      },
      "source": [
        "**This parts helps by checking if the code worked.**\n",
        "**Check if you get similar results, if not repair your code**\n"
      ]
    }
  ]
}